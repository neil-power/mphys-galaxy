{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GZDESI/GZRings/GZCD not available from galaxy_datasets.pytorch.datasets - skipping\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "import pandas as pd\n",
    "from torchinfo import summary\n",
    "import torchvision.models as models\n",
    "from dataset_utils import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from matplotlib.offsetbox import (OffsetImage, AnnotationBbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test_model= models.resnet18(weights=None, num_classes = 3)\n",
    "state_dict = torch.load('../Metrics/resnet18_cut_dataset_repeat/version_0/model.pt')\n",
    "# create new OrderedDict that does not contain `module.`\n",
    "from collections import OrderedDict\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    name = k[6:] # remove `module.`\n",
    "    new_state_dict[name] = v\n",
    "test_model.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 1 galaxy filepaths\n"
     ]
    }
   ],
   "source": [
    "LOCAL_SUBSET_DATA_PATH =  \"../Data/Subset\"\n",
    "catalog = pd.read_csv( \"../Data/gz1_desi_cross_cat_local_subset.csv\")[0:1]\n",
    "catalog[\"file_loc\"] = get_file_paths(catalog,LOCAL_SUBSET_DATA_PATH)\n",
    "\n",
    "datamodule = GalaxyDataModule(\n",
    "            label_cols=None,\n",
    "            predict_catalog=catalog,\n",
    "            custom_albumentation_transform=generate_transforms(resize_after_crop=160),\n",
    "            batch_size=60, num_workers=4,\n",
    "            )\n",
    "datamodule.prepare_data()\n",
    "datamodule.setup(stage='predict')\n",
    "image=torch.from_numpy(datamodule.predict_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0456, 0.0971, 0.0680,  ..., 0.0975, 0.1140, 0.1624],\n",
       "         [0.0647, 0.0704, 0.0030,  ..., 0.1033, 0.1178, 0.1199],\n",
       "         [0.0843, 0.0520, 0.0569,  ..., 0.0412, 0.0829, 0.1084],\n",
       "         ...,\n",
       "         [0.0202, 0.0440, 0.0324,  ..., 0.1047, 0.0981, 0.1008],\n",
       "         [0.0162, 0.0665, 0.0818,  ..., 0.0755, 0.0976, 0.0806],\n",
       "         [0.0575, 0.0867, 0.0288,  ..., 0.1199, 0.0596, 0.0816]],\n",
       "\n",
       "        [[0.0285, 0.0801, 0.0510,  ..., 0.0987, 0.0759, 0.1009],\n",
       "         [0.0782, 0.0761, 0.0007,  ..., 0.0839, 0.0757, 0.0724],\n",
       "         [0.1097, 0.0693, 0.0551,  ..., 0.1032, 0.1067, 0.1106],\n",
       "         ...,\n",
       "         [0.0262, 0.0232, 0.0716,  ..., 0.1048, 0.0764, 0.0787],\n",
       "         [0.0290, 0.0578, 0.1156,  ..., 0.0677, 0.0810, 0.0649],\n",
       "         [0.0776, 0.1020, 0.0565,  ..., 0.0944, 0.0446, 0.0721]],\n",
       "\n",
       "        [[0.0253, 0.0847, 0.0556,  ..., 0.1174, 0.0917, 0.1016],\n",
       "         [0.0645, 0.0742, 0.0010,  ..., 0.1056, 0.0895, 0.0732],\n",
       "         [0.0932, 0.0587, 0.0560,  ..., 0.0974, 0.0993, 0.0950],\n",
       "         ...,\n",
       "         [0.0416, 0.0286, 0.0447,  ..., 0.1127, 0.0921, 0.0857],\n",
       "         [0.0404, 0.0588, 0.0907,  ..., 0.0753, 0.0876, 0.0688],\n",
       "         [0.0741, 0.0898, 0.0353,  ..., 0.0949, 0.0479, 0.0791]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mask(s, margin=2, dtype=torch.float32):\n",
    "    mask = torch.zeros(1, 1, s, s, dtype=dtype)\n",
    "    c = (s-1) / 2\n",
    "    t = (c - margin/100.*c)**2\n",
    "    sig = 2.\n",
    "    for x in range(s):\n",
    "        for y in range(s):\n",
    "            r = (x - c) ** 2 + (y - c) ** 2\n",
    "            if r > t:\n",
    "                mask[..., x, y] = math.exp((t - r)/sig**2)\n",
    "            else:\n",
    "                mask[..., x, y] = 1.\n",
    "    return mask\n",
    "    \n",
    "def positionimage(x, y, ax, ar, zoom=0.5):\n",
    "    \"\"\"Place image from file `fname` into axes `ax` at position `x,y`.\"\"\"\n",
    "    \n",
    "    imsize = ar.shape[0]\n",
    "    if imsize==151: zoom=0.24\n",
    "    if imsize==51: zoom = 0.75\n",
    "    im = OffsetImage(ar, zoom=zoom)\n",
    "    im.image.axes = ax\n",
    "    \n",
    "    ab = AnnotationBbox(im, (x,y), xycoords='data')\n",
    "    ax.add_artist(ab)\n",
    "    \n",
    "    return\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def make_linemarker(x,y,dx,col,ax):\n",
    "    \n",
    "    xs = [x-0.5*dx,x+0.5*dx]\n",
    "    for i in range(0,y.shape[0]):\n",
    "        ys = [y[i],y[i]]\n",
    "        ax.plot(xs,ys,marker=\",\",c=col,alpha=0.1,lw=5)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlapping(x, y, beta=0.1):\n",
    "\n",
    "    n_z = 100\n",
    "    z = np.linspace(0,1,n_z)\n",
    "    dz = 1./n_z\n",
    "    \n",
    "    norm = 1./(beta*np.sqrt(2*np.pi))\n",
    "    \n",
    "    n_x = len(x)\n",
    "    f_x = np.zeros(n_z)\n",
    "    for i in range(n_z):\n",
    "        for j in range(n_x):\n",
    "            f_x[i] += norm*np.exp(-0.5*(z[i] - x[j])**2/beta**2)\n",
    "        f_x[i] /= n_x\n",
    "    \n",
    "    \n",
    "    n_y = len(y)\n",
    "    f_y = np.zeros(n_z)\n",
    "    for i in range(n_z):\n",
    "        for j in range(n_y):\n",
    "            f_y[i] += norm*np.exp(-0.5*(z[i] - y[j])**2/beta**2)\n",
    "            \n",
    "        f_y[i] /= n_y\n",
    "    \n",
    "    \n",
    "    eta_z = np.zeros(n_z)\n",
    "    eta_z = np.minimum(f_x, f_y)\n",
    "        \n",
    "    # pl.subplot(111)\n",
    "    # pl.plot(z, f_x, label=r\"$f_x$\")\n",
    "    # pl.plot(z, f_y, label=r\"$f_y$\")\n",
    "    # pl.plot(z, eta_z, label=r\"$\\eta_z$\")\n",
    "    # pl.legend()\n",
    "    # pl.show()\n",
    "    # print('meow')\n",
    "\n",
    "    return np.sum(eta_z)*dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fr_rotation_test(model, data, target=None, idx=1, device='cpu'):\n",
    "    #model is the model, data is one image, idx??\n",
    "    T = 100 #number of passes\n",
    "    rotation_list = range(0, 180, 20)\n",
    "    #print(\"True classification: \",target[0].item())\n",
    "    \n",
    "    image_list = []\n",
    "    outp_list = []\n",
    "    inpt_list = []\n",
    "    for r in rotation_list:\n",
    "        \n",
    "        # make rotated image:\n",
    "        rotation_matrix = torch.Tensor([[[np.cos(r/360.0*2*np.pi), -np.sin(r/360.0*2*np.pi), 0],\n",
    "                                         [np.sin(r/360.0*2*np.pi), np.cos(r/360.0*2*np.pi), 0]]]).to(device)\n",
    "        grid = F.affine_grid(rotation_matrix, data.size(), align_corners=True) #data.size()\n",
    "        data_rotate = F.grid_sample(data, grid, align_corners=True)\n",
    "        image_list.append(data_rotate)\n",
    "        \n",
    "        # get straight prediction:\n",
    "        model.eval()\n",
    "        x = model(data_rotate)\n",
    "        p = F.softmax(x,dim=1)\n",
    "                                         \n",
    "        # run 100 stochastic forward passes:\n",
    "        model.enable_dropout()\n",
    "        output_list, input_list = [], []\n",
    "        for i in range(T):\n",
    "            x = model(data_rotate)\n",
    "            input_list.append(torch.unsqueeze(x, 0).cpu())\n",
    "            output_list.append(torch.unsqueeze(F.softmax(x,dim=1), 0).cpu())\n",
    "                                         \n",
    "        # calculate the mean output for each target:\n",
    "        output_mean = np.squeeze(torch.cat(output_list, 0).mean(0).data.cpu().numpy())\n",
    "                                             \n",
    "        # append per rotation output into list:\n",
    "        outp_list.append(np.squeeze(torch.cat(output_list, 0).data.numpy()))\n",
    "        inpt_list.append(np.squeeze(torch.cat(input_list, 0).data.numpy()))\n",
    "\n",
    "        #print ('rotation degree', str(r), 'Predict : {} - {}'.format(output_mean.argmax(),output_mean))\n",
    "\n",
    "    preds = np.array([0,1])\n",
    "    classes = np.array([\"FRI\",\"FRII\"])\n",
    "    \n",
    "    outp_list = np.array(outp_list)\n",
    "    inpt_list = np.array(inpt_list)\n",
    "    rotation_list = np.array(rotation_list)\n",
    "\n",
    "    colours=[\"b\",\"r\"]\n",
    "\n",
    "    #fig1, (a0, a1) = pl.subplots(2, 1, gridspec_kw={'height_ratios': [8,1]})\n",
    "    fig2, (a2, a3) = pl.subplots(2, 1, gridspec_kw={'height_ratios': [8,1]})\n",
    "\n",
    "    eta = np.zeros(len(rotation_list))\n",
    "    for i in range(len(rotation_list)):\n",
    "        x = outp_list[i,:,0]\n",
    "        y = outp_list[i,:,1]\n",
    "        eta[i] = overlapping(x, y)\n",
    "\n",
    "    #a0.set_title(\"Input\")\n",
    "    if np.mean(eta)>=0.01:\n",
    "        a2.set_title(r\"$\\langle \\eta \\rangle = $ {:.2f}\".format(np.mean(eta)))\n",
    "    else:\n",
    "        a2.set_title(r\"$\\langle \\eta \\rangle < 0.01$\")\n",
    "\n",
    "    dx = 0.8*(rotation_list[1]-rotation_list[0])\n",
    "    for pred in preds:\n",
    "        col = colours[pred]\n",
    "        #a0.plot(rotation_list[0],inpt_list[0,0,pred],marker=\",\",c=col,label=str(pred))\n",
    "        a2.plot(rotation_list[0],outp_list[0,0,pred],marker=\",\",c=col,label=classes[pred])\n",
    "        for i in range(rotation_list.shape[0]):\n",
    "        #    make_linemarker(rotation_list[i],inpt_list[i,:,pred],dx,col,a0)\n",
    "            make_linemarker(rotation_list[i],outp_list[i,:,pred],dx,col,a2)\n",
    "        \n",
    "    #a2.plot(rotation_list, eta)\n",
    "    \n",
    "    #a0.legend()\n",
    "    a2.legend(loc='center right')\n",
    "    #a0.axis([0,180,0,1])\n",
    "    #a0.set_xlabel(\"Rotation [deg]\")\n",
    "    a2.set_xlabel(\"Rotation [deg]\")\n",
    "    #a1.axis([0,180,0,1])\n",
    "    a3.axis([0,180,0,1])\n",
    "    #a1.axis('off')\n",
    "    a3.axis('off')\n",
    "    \n",
    "    imsize = data.size()[2]\n",
    "    mask = build_mask(imsize, margin=1)\n",
    "            \n",
    "    for i in range(len(rotation_list)):\n",
    "        inc = 0.5*(180./len(rotation_list))\n",
    "        #positionimage(rotation_list[i]+inc, 0., a1, image_list[i][0, 0, :, :].data.numpy(), zoom=0.32)\n",
    "        positionimage(rotation_list[i]+inc, 0., a3, mask[0,0,:,:]*image_list[i][0, 0, :, :].data.cpu().numpy(), zoom=0.32)\n",
    "        \n",
    "    \n",
    "    #fig1.tight_layout()\n",
    "    fig2.tight_layout()\n",
    "\n",
    "    #fig1.subplots_adjust(bottom=0.15)\n",
    "    fig2.subplots_adjust(bottom=0.15)\n",
    "\n",
    "    #pl.show()\n",
    "    fig2.savefig(\"test\"+str(idx)+\".png\")\n",
    "    \n",
    "    pl.close()\n",
    "    \n",
    "    return np.mean(eta), np.std(eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "affine_grid only supports 4D and 5D sizes, for 2D and 3D affine transforms, respectively. Got size torch.Size([3, 160, 160]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m fr_rotation_test(test_model, image)\n",
      "Cell \u001b[1;32mIn[7], line 15\u001b[0m, in \u001b[0;36mfr_rotation_test\u001b[1;34m(model, data, target, idx, device)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rotation_list:\n\u001b[0;32m     11\u001b[0m     \n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# make rotated image:\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     rotation_matrix \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor([[[np\u001b[38;5;241m.\u001b[39mcos(r\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m360.0\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mpi), \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39msin(r\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m360.0\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mpi), \u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m     14\u001b[0m                                      [np\u001b[38;5;241m.\u001b[39msin(r\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m360.0\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mpi), np\u001b[38;5;241m.\u001b[39mcos(r\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m360.0\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mpi), \u001b[38;5;241m0\u001b[39m]]])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 15\u001b[0m     grid \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39maffine_grid(rotation_matrix, data\u001b[38;5;241m.\u001b[39msize(), align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m#data.size()\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     data_rotate \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mgrid_sample(data, grid, align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     17\u001b[0m     image_list\u001b[38;5;241m.\u001b[39mappend(data_rotate)\n",
      "File \u001b[1;32mc:\\Users\\Ezzy\\miniconda3\\envs\\Galaxy\\Lib\\site-packages\\torch\\nn\\functional.py:4402\u001b[0m, in \u001b[0;36maffine_grid\u001b[1;34m(theta, size, align_corners)\u001b[0m\n\u001b[0;32m   4400\u001b[0m     spatial_size \u001b[38;5;241m=\u001b[39m size[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m:]  \u001b[38;5;66;03m# spatial dimension sizes\u001b[39;00m\n\u001b[0;32m   4401\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4402\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m   4403\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maffine_grid only supports 4D and 5D sizes, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4404\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor 2D and 3D affine transforms, respectively. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4405\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4406\u001b[0m     )\n\u001b[0;32m   4407\u001b[0m \u001b[38;5;66;03m# check for empty span\u001b[39;00m\n\u001b[0;32m   4408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m align_corners \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(spatial_size) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: affine_grid only supports 4D and 5D sizes, for 2D and 3D affine transforms, respectively. Got size torch.Size([3, 160, 160])."
     ]
    }
   ],
   "source": [
    "fr_rotation_test(test_model, image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Galaxy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
