{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E2CNN example function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "from escnn import gspaces\n",
    "import torch.nn as nn\n",
    "import escnn.nn as enn\n",
    "from torch import Tensor\n",
    "from typing import Type, Any, Callable, Union, List, Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 45 deg rotational model\n",
    "class C8SteerableCNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes=10):\n",
    "        \n",
    "        super(C8SteerableCNN, self).__init__()\n",
    "        \n",
    "        # the model is equivariant under rotations by 45 degrees, modelled by C8\n",
    "        self.r2_act = gspaces.rot2dOnR2(N=8)\n",
    "        \n",
    "        # the input image is a scalar field, corresponding to the trivial representation\n",
    "        in_type = enn.FieldType(self.r2_act, [self.r2_act.trivial_repr])\n",
    "        \n",
    "        # we store the input type for wrapping the images into a geometric tensor during the forward pass\n",
    "        self.input_type = in_type\n",
    "        \n",
    "        # convolution 1\n",
    "        # first specify the output type of the convolutional layer\n",
    "        # we choose 24 feature fields, each transforming under the regular representation of C8\n",
    "        out_type = enn.FieldType(self.r2_act, 24*[self.r2_act.regular_repr])\n",
    "        self.block1 = enn.SequentialModule(\n",
    "            enn.MaskModule(in_type, 29, margin=1),\n",
    "            enn.R2Conv(in_type, out_type, kernel_size=7, padding=1, bias=False),\n",
    "            enn.InnerBatchNorm(out_type),\n",
    "            enn.ReLU(out_type, inplace=True)\n",
    "        )\n",
    "        \n",
    "        # convolution 2\n",
    "        # the old output type is the input type to the next layer\n",
    "        in_type = self.block1.out_type\n",
    "        # the output type of the second convolution layer are 48 regular feature fields of C8\n",
    "        out_type = enn.FieldType(self.r2_act, 48*[self.r2_act.regular_repr])\n",
    "        self.block2 = enn.SequentialModule(\n",
    "            enn.R2Conv(in_type, out_type, kernel_size=5, padding=2, bias=False),\n",
    "            enn.InnerBatchNorm(out_type),\n",
    "            enn.ReLU(out_type, inplace=True)\n",
    "        )\n",
    "        self.pool1 = enn.SequentialModule(\n",
    "            enn.PointwiseAvgPoolAntialiased(out_type, sigma=0.66, stride=2)\n",
    "        )\n",
    "        \n",
    "        # convolution 3\n",
    "        # the old output type is the input type to the next layer\n",
    "        in_type = self.block2.out_type\n",
    "        # the output type of the third convolution layer are 48 regular feature fields of C8\n",
    "        out_type = enn.FieldType(self.r2_act, 48*[self.r2_act.regular_repr])\n",
    "        self.block3 = enn.SequentialModule(\n",
    "            enn.R2Conv(in_type, out_type, kernel_size=5, padding=2, bias=False),\n",
    "            enn.InnerBatchNorm(out_type),\n",
    "            enn.ReLU(out_type, inplace=True)\n",
    "        )\n",
    "        \n",
    "        # convolution 4\n",
    "        # the old output type is the input type to the next layer\n",
    "        in_type = self.block3.out_type\n",
    "        # the output type of the fourth convolution layer are 96 regular feature fields of C8\n",
    "        out_type = enn.FieldType(self.r2_act, 96*[self.r2_act.regular_repr])\n",
    "        self.block4 = enn.SequentialModule(\n",
    "            enn.R2Conv(in_type, out_type, kernel_size=5, padding=2, bias=False),\n",
    "            enn.InnerBatchNorm(out_type),\n",
    "            enn.ReLU(out_type, inplace=True)\n",
    "        )\n",
    "        self.pool2 = enn.SequentialModule(\n",
    "            enn.PointwiseAvgPoolAntialiased(out_type, sigma=0.66, stride=2)\n",
    "        )\n",
    "        \n",
    "        # convolution 5\n",
    "        # the old output type is the input type to the next layer\n",
    "        in_type = self.block4.out_type\n",
    "        # the output type of the fifth convolution layer are 96 regular feature fields of C8\n",
    "        out_type = enn.FieldType(self.r2_act, 96*[self.r2_act.regular_repr])\n",
    "        self.block5 = enn.SequentialModule(\n",
    "            enn.R2Conv(in_type, out_type, kernel_size=5, padding=2, bias=False),\n",
    "            enn.InnerBatchNorm(out_type),\n",
    "            enn.ReLU(out_type, inplace=True)\n",
    "        )\n",
    "        \n",
    "        # convolution 6\n",
    "        # the old output type is the input type to the next layer\n",
    "        in_type = self.block5.out_type\n",
    "        # the output type of the sixth convolution layer are 64 regular feature fields of C8\n",
    "        out_type = enn.FieldType(self.r2_act, 64*[self.r2_act.regular_repr])\n",
    "        self.block6 = enn.SequentialModule(\n",
    "            enn.R2Conv(in_type, out_type, kernel_size=5, padding=1, bias=False),\n",
    "            enn.InnerBatchNorm(out_type),\n",
    "            enn.ReLU(out_type, inplace=True)\n",
    "        )\n",
    "        self.pool3 = enn.PointwiseAvgPoolAntialiased(out_type, sigma=0.66, stride=1, padding=0)\n",
    "        \n",
    "        self.gpool = enn.GroupPooling(out_type)\n",
    "        \n",
    "        # number of output channels\n",
    "        c = self.gpool.out_type.size\n",
    "        \n",
    "        # Fully Connected\n",
    "        self.fully_net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(c, 64),\n",
    "            torch.nn.BatchNorm1d(64),\n",
    "            torch.nn.ELU(inplace=True),\n",
    "            torch.nn.Linear(64, n_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, input: torch.Tensor):\n",
    "        # wrap the input tensor in a GeometricTensor\n",
    "        # (associate it with the input type)\n",
    "        x = enn.GeometricTensor(input, self.input_type)\n",
    "        \n",
    "        # apply each equivariant block\n",
    "        \n",
    "        # Each layer has an input and an output type\n",
    "        # A layer takes a GeometricTensor in input.\n",
    "        # This tensor needs to be associated with the same representation of the layer's input type\n",
    "        #\n",
    "        # The Layer outputs a new GeometricTensor, associated with the layer's output type.\n",
    "        # As a result, consecutive layers need to have matching input/output types\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.block5(x)\n",
    "        x = self.block6(x)\n",
    "        \n",
    "        # pool over the spatial dimensions\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        # pool over the group\n",
    "        x = self.gpool(x)\n",
    "\n",
    "        # unwrap the output GeometricTensor\n",
    "        # (take the Pytorch tensor and discard the associated representation)\n",
    "        x = x.tensor\n",
    "        \n",
    "        # classify with the final fully connected layers)\n",
    "        x = self.fully_net(x.reshape(x.shape[0], -1))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiii = C8SteerableCNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet version - taken from ReResnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regular_feature_type(gspace: gspaces.GSpace, planes: int, fixparams: bool = False):\n",
    "    \"\"\" build a regular feature map with the specified number of channels\"\"\"\n",
    "    assert gspace.fibergroup.order() > 0\n",
    "\n",
    "    N = gspace.fibergroup.order()\n",
    "\n",
    "    if fixparams:\n",
    "        planes *= math.sqrt(N)\n",
    "\n",
    "    planes = planes / N\n",
    "    planes = int(planes)\n",
    "\n",
    "    return enn.FieldType(gspace, [gspace.regular_repr] * planes)\n",
    "\n",
    "\n",
    "def trivial_feature_type(gspace: gspaces.GSpace, planes: int, fixparams: bool = False):\n",
    "    \"\"\" build a trivial feature map with the specified number of channels\"\"\"\n",
    "\n",
    "    if fixparams:\n",
    "        planes *= math.sqrt(gspace.fibergroup.order())\n",
    "\n",
    "    planes = int(planes)\n",
    "    return enn.FieldType(gspace, [gspace.trivial_repr] * planes)\n",
    "\n",
    "\n",
    "FIELD_TYPE = {\n",
    "    \"trivial\": trivial_feature_type,\n",
    "    \"regular\": regular_feature_type,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(gspace, inplanes, out_planes, stride=1, padding=1, dilation=1, bias=False, fixparams=False):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    in_type = FIELD_TYPE['regular'](gspace, inplanes, fixparams=fixparams)\n",
    "    out_type = FIELD_TYPE['regular'](gspace, out_planes, fixparams=fixparams)\n",
    "    return enn.R2Conv(in_type, out_type, 3,\n",
    "                      stride=stride,\n",
    "                      padding=padding,\n",
    "                      dilation=dilation,\n",
    "                      bias=bias,\n",
    "                      sigma=None,\n",
    "                      frequencies_cutoff=lambda r: 3 * r)\n",
    "\n",
    "\n",
    "def conv1x1(gspace, inplanes, out_planes, stride=1, padding=0, dilation=1, bias=False, fixparams=False):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    in_type = FIELD_TYPE['regular'](gspace, inplanes, fixparams=fixparams)\n",
    "    out_type = FIELD_TYPE['regular'](gspace, out_planes, fixparams=fixparams)\n",
    "    return enn.R2Conv(in_type, out_type, 1,\n",
    "                      stride=stride,\n",
    "                      padding=padding,\n",
    "                      dilation=dilation,\n",
    "                      bias=bias,\n",
    "                      sigma=None,\n",
    "                      frequencies_cutoff=lambda r: 3 * r)\n",
    "\n",
    "def build_norm_layer(cfg, gspace, num_features, postfix=''):\n",
    "    in_type = FIELD_TYPE['regular'](gspace, num_features)\n",
    "    return 'bn' + str(postfix), enn.InnerBatchNorm(in_type)\n",
    "\n",
    "class BasicBlock(enn.EquivariantModule):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 expansion=1,\n",
    "                 stride=1,\n",
    "                 dilation=1,\n",
    "                 downsample=None,\n",
    "                 style='pytorch',\n",
    "                 with_cp=False,\n",
    "                 conv_cfg=None,\n",
    "                 norm_cfg=dict(type='BN'),\n",
    "                 gspace=None,\n",
    "                 fixparams=False):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.in_type = FIELD_TYPE['regular'](\n",
    "            gspace, in_channels, fixparams=fixparams)\n",
    "        self.out_type = FIELD_TYPE['regular'](\n",
    "            gspace, out_channels, fixparams=fixparams)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.expansion = expansion\n",
    "        assert self.expansion == 1\n",
    "        assert out_channels % expansion == 0\n",
    "        self.mid_channels = out_channels // expansion\n",
    "        self.stride = stride\n",
    "        self.dilation = dilation\n",
    "        self.style = style\n",
    "        self.with_cp = with_cp\n",
    "        self.conv_cfg = conv_cfg\n",
    "        self.norm_cfg = norm_cfg\n",
    "\n",
    "        self.norm1_name, norm1 = build_norm_layer(\n",
    "            norm_cfg, gspace, self.mid_channels, postfix=1)\n",
    "        self.norm2_name, norm2 = build_norm_layer(\n",
    "            norm_cfg, gspace, out_channels, postfix=2)\n",
    "\n",
    "        self.conv1 = conv3x3(\n",
    "            gspace,\n",
    "            in_channels,\n",
    "            self.mid_channels,\n",
    "            stride=stride,\n",
    "            padding=dilation,\n",
    "            dilation=dilation,\n",
    "            bias=False,\n",
    "            fixparams=fixparams)\n",
    "        self.add_module(self.norm1_name, norm1)\n",
    "        self.relu1 = enn.ReLU(self.conv1.out_type, inplace=True)\n",
    "        self.conv2 = conv3x3(\n",
    "            gspace,\n",
    "            self.mid_channels,\n",
    "            out_channels,\n",
    "            padding=1,\n",
    "            bias=False,\n",
    "            fixparams=fixparams)\n",
    "        self.add_module(self.norm2_name, norm2)\n",
    "\n",
    "        self.relu2 = enn.ReLU(self.conv1.out_type, inplace=True)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    @property\n",
    "    def norm1(self):\n",
    "        return getattr(self, self.norm1_name)\n",
    "\n",
    "    @property\n",
    "    def norm2(self):\n",
    "        return getattr(self, self.norm2_name)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        def _inner_forward(x):\n",
    "            identity = x\n",
    "\n",
    "            out = self.conv1(x)\n",
    "            out = self.norm1(out)\n",
    "            out = self.relu1(out)\n",
    "\n",
    "            out = self.conv2(out)\n",
    "            out = self.norm2(out)\n",
    "\n",
    "            if self.downsample is not None:\n",
    "                identity = self.downsample(x)\n",
    "\n",
    "            out += identity\n",
    "\n",
    "            return out\n",
    "\n",
    "        if self.with_cp and x.requires_grad:\n",
    "            print('add me back')\n",
    "            #out = cp.checkpoint(_inner_forward, x)\n",
    "        else:\n",
    "            out = _inner_forward(x)\n",
    "\n",
    "        out = self.relu2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(enn.EquivariantModule):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 expansion=4,\n",
    "                 stride=1,\n",
    "                 dilation=1,\n",
    "                 downsample=None,\n",
    "                 style='pytorch',\n",
    "                 with_cp=False,\n",
    "                 conv_cfg=None,\n",
    "                 norm_cfg=dict(type='BN'),\n",
    "                 gspace=None,\n",
    "                 fixparams=False):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        assert style in ['pytorch', 'caffe']\n",
    "        self.in_type = FIELD_TYPE['regular'](\n",
    "            gspace, in_channels, fixparams=fixparams)\n",
    "        self.out_type = FIELD_TYPE['regular'](\n",
    "            gspace, out_channels, fixparams=fixparams)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.expansion = expansion\n",
    "        assert out_channels % expansion == 0\n",
    "        self.mid_channels = out_channels // expansion\n",
    "        self.stride = stride\n",
    "        self.dilation = dilation\n",
    "        self.style = style\n",
    "        self.with_cp = with_cp\n",
    "        self.conv_cfg = conv_cfg\n",
    "        self.norm_cfg = norm_cfg\n",
    "        if self.style == 'pytorch':\n",
    "            self.conv1_stride = 1\n",
    "            self.conv2_stride = stride\n",
    "        else:\n",
    "            self.conv1_stride = stride\n",
    "            self.conv2_stride = 1\n",
    "\n",
    "        self.norm1_name, norm1 = build_norm_layer(\n",
    "            norm_cfg, gspace, self.mid_channels, postfix=1)\n",
    "        self.norm2_name, norm2 = build_norm_layer(\n",
    "            norm_cfg, gspace, self.mid_channels, postfix=2)\n",
    "        self.norm3_name, norm3 = build_norm_layer(\n",
    "            norm_cfg, gspace, out_channels, postfix=3)\n",
    "\n",
    "        self.conv1 = conv1x1(\n",
    "            gspace,\n",
    "            in_channels,\n",
    "            self.mid_channels,\n",
    "            stride=self.conv1_stride,\n",
    "            bias=False,\n",
    "            fixparams=fixparams)\n",
    "        self.add_module(self.norm1_name, norm1)\n",
    "        self.relu1 = enn.ReLU(self.conv1.out_type, inplace=True)\n",
    "        self.conv2 = conv3x3(\n",
    "            gspace,\n",
    "            self.mid_channels,\n",
    "            self.mid_channels,\n",
    "            stride=self.conv2_stride,\n",
    "            padding=dilation,\n",
    "            dilation=dilation,\n",
    "            bias=False,\n",
    "            fixparams=fixparams)\n",
    "\n",
    "        self.add_module(self.norm2_name, norm2)\n",
    "        self.relu2 = enn.ReLU(self.conv2.out_type, inplace=True)\n",
    "        self.conv3 = conv1x1(\n",
    "            gspace,\n",
    "            self.mid_channels,\n",
    "            out_channels,\n",
    "            bias=False,\n",
    "            fixparams=fixparams)\n",
    "        self.add_module(self.norm3_name, norm3)\n",
    "        self.relu3 = enn.ReLU(self.conv3.out_type, inplace=True)\n",
    "\n",
    "        self.downsample = downsample\n",
    "\n",
    "    @property\n",
    "    def norm1(self):\n",
    "        return getattr(self, self.norm1_name)\n",
    "\n",
    "    @property\n",
    "    def norm2(self):\n",
    "        return getattr(self, self.norm2_name)\n",
    "\n",
    "    @property\n",
    "    def norm3(self):\n",
    "        return getattr(self, self.norm3_name)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        def _inner_forward(x):\n",
    "            identity = x\n",
    "\n",
    "            out = self.conv1(x)\n",
    "            out = self.norm1(out)\n",
    "            out = self.relu1(out)\n",
    "\n",
    "            out = self.conv2(out)\n",
    "            out = self.norm2(out)\n",
    "            out = self.relu2(out)\n",
    "\n",
    "            out = self.conv3(out)\n",
    "            out = self.norm3(out)\n",
    "\n",
    "            if self.downsample is not None:\n",
    "                identity = self.downsample(x)\n",
    "\n",
    "            out += identity\n",
    "\n",
    "            return out\n",
    "\n",
    "        if self.with_cp and x.requires_grad:\n",
    "            print('hi its me, im the issue')\n",
    "            #out = cp.checkpoint(_inner_forward, x)\n",
    "        else:\n",
    "            out = _inner_forward(x)\n",
    "\n",
    "        out = self.relu3(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "def get_expansion(block, expansion=None):\n",
    "    if isinstance(expansion, int):\n",
    "        assert expansion > 0\n",
    "    elif expansion is None:\n",
    "        if hasattr(block, 'expansion'):\n",
    "            expansion = block.expansion\n",
    "        elif issubclass(block, BasicBlock):\n",
    "            expansion = 1\n",
    "        elif issubclass(block, Bottleneck):\n",
    "            expansion = 4\n",
    "        else:\n",
    "            raise TypeError(f'expansion is not specified for {block.__name__}')\n",
    "    else:\n",
    "        raise TypeError('expansion must be an integer or None')\n",
    "\n",
    "    return expansion\n",
    "\n",
    "class ResLayer(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 block,\n",
    "                 num_blocks,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 expansion=None,\n",
    "                 stride=1,\n",
    "                 avg_down=False,\n",
    "                 conv_cfg=None,\n",
    "                 norm_cfg=dict(type='BN'),\n",
    "                 gspace=None,\n",
    "                 fixparams=False,\n",
    "                 **kwargs):\n",
    "        self.block = block\n",
    "        self.expansion = get_expansion(block, expansion)\n",
    "\n",
    "        downsample = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            downsample = []\n",
    "            conv_stride = stride\n",
    "            if avg_down and stride != 1:\n",
    "                conv_stride = 1\n",
    "                in_type = FIELD_TYPE[\"regular\"](\n",
    "                    gspace, in_channels, fixparams=fixparams)\n",
    "                downsample.append(\n",
    "                    enn.PointwiseAvgPool(\n",
    "                        in_type,\n",
    "                        kernel_size=stride,\n",
    "                        stride=stride,\n",
    "                        ceil_mode=True))\n",
    "            downsample.extend([\n",
    "                conv1x1(gspace, in_channels, out_channels,\n",
    "                        stride=conv_stride, bias=False),\n",
    "                build_norm_layer(norm_cfg, gspace, out_channels)[1]\n",
    "            ])\n",
    "            downsample = enn.SequentialModule(*downsample)\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                expansion=self.expansion,\n",
    "                stride=stride,\n",
    "                downsample=downsample,\n",
    "                conv_cfg=conv_cfg,\n",
    "                norm_cfg=norm_cfg,\n",
    "                gspace=gspace,\n",
    "                fixparams=fixparams,\n",
    "                **kwargs))\n",
    "        in_channels = out_channels\n",
    "        for i in range(1, num_blocks):\n",
    "            layers.append(\n",
    "                block(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    expansion=self.expansion,\n",
    "                    stride=1,\n",
    "                    conv_cfg=conv_cfg,\n",
    "                    norm_cfg=norm_cfg,\n",
    "                    gspace=gspace,\n",
    "                    fixparams=fixparams,\n",
    "                    **kwargs))\n",
    "        super(ResLayer, self).__init__(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReResNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        block: Type[Union[BasicBlock, Bottleneck]],\n",
    "        layers: List[int],\n",
    "        num_channels: int = 3,\n",
    "        num_classes: int = 1000,\n",
    "        use_max_pool: bool = False,\n",
    "        use_avg_pool: bool = True,\n",
    "        avg_pool_size: Tuple[int] = (4, 4),\n",
    "        add_fc: Optional[List[int]] = None,\n",
    "        zero_init_residual: bool = False,\n",
    "        groups: int = 1,\n",
    "        width_per_group: int = 64,\n",
    "        replace_stride_with_dilation: Optional[List[bool]] = None,\n",
    "        norm_layer: Optional[Callable[..., torch.nn.Module]] = None\n",
    "    ) -> None:\n",
    "        super(ReResNet, self).__init__()\n",
    "\n",
    "        self.r2_act = gspaces.rot2dOnR2(8)\n",
    "        \n",
    "        in_type = enn.FieldType(self.r2_act, [self.r2_act.trivial_repr])\n",
    "        self.input_type = in_type\n",
    "        \n",
    "        out_type = enn.FieldType(self.r2_act, 6*[self.r2_act.regular_repr])\n",
    "\n",
    "        if norm_layer is None:\n",
    "            norm_layer = torch.nn.BatchNorm2d ########\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64 ######\n",
    "        self.dilation = 1 \n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(num_channels, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(avg_pool_size)\n",
    "        pool_expansion = 1\n",
    "        if not use_avg_pool:\n",
    "            pool_expansion = 16 if use_max_pool else 64\n",
    "        else:\n",
    "            pool_expansion = np.prod(avg_pool_size)\n",
    "        self.fc = self._make_fc(512 * block.expansion * pool_expansion, num_classes, add_fc)\n",
    "\n",
    "        self.use_max_pool = use_max_pool\n",
    "        self.use_avg_pool = use_avg_pool\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_fc(in_features: int, out_features: int, add_fc: Optional[List[int]]):\n",
    "        if add_fc is None:\n",
    "            return nn.Linear(in_features, out_features)\n",
    "        else:\n",
    "            add_fc.insert(0, in_features)\n",
    "            add_fc.append(out_features)\n",
    "            fc_layers = []\n",
    "            for i in range(len(add_fc) - 1):\n",
    "                fc_layers.append(nn.Linear(add_fc[i], add_fc[i + 1]))\n",
    "                if i != len(add_fc) - 2:\n",
    "                    fc_layers.append(nn.Tanh())\n",
    "            return nn.Sequential(*fc_layers)\n",
    "\n",
    "    def _make_layer(self, block: Type[Union[BasicBlock, Bottleneck]], planes: int, blocks: int,\n",
    "                    stride: int = 1, dilate: bool = False) -> nn.Sequential:\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        if self.use_max_pool:\n",
    "            x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        if self.use_avg_pool:\n",
    "            x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "    \n",
    "def _resnet(\n",
    "    arch: str,\n",
    "    block: Type[Union[BasicBlock, Bottleneck]],\n",
    "    layers: List[int],\n",
    "    pretrained: bool,\n",
    "    progress: bool,\n",
    "    **kwargs: Any\n",
    ") -> ResNet:\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "    if pretrained:\n",
    "        raise NotImplementedError\n",
    "        \"\"\"state_dict = load_state_dict_from_url(model_urls[arch],\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict)\"\"\"\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Galaxy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
