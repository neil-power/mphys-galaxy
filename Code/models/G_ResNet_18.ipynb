{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Any, Union\n",
    "\n",
    "import e2cnn.nn as enn\n",
    "from e2cnn import gspaces\n",
    "from e2cnn.nn import init\n",
    "from e2cnn.nn import GeometricTensor\n",
    "from e2cnn.nn import FieldType\n",
    "from e2cnn.nn import EquivariantModule\n",
    "from e2cnn.gspaces import *\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "def conv3x3(in_type: enn.FieldType, out_type: enn.FieldType, stride=1, padding=1,\n",
    "            dilation=1, bias=False):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return enn.R2Conv(in_type, out_type, 3,\n",
    "                      stride=stride,\n",
    "                      padding=padding,\n",
    "                      dilation=dilation,\n",
    "                      bias=bias,\n",
    "                      sigma=None,\n",
    "                      frequencies_cutoff=lambda r: 3*r,\n",
    "                      )\n",
    "\n",
    "def conv1x1(in_type: enn.FieldType, out_type: enn.FieldType, stride=1, padding=0,\n",
    "            dilation=1, bias=False):\n",
    "    \"\"\"1x1 convolution with padding\"\"\"\n",
    "    return enn.R2Conv(in_type, out_type, 1,\n",
    "                      stride=stride,\n",
    "                      padding=padding,\n",
    "                      dilation=dilation,\n",
    "                      bias=bias,\n",
    "                      sigma=None,\n",
    "                      frequencies_cutoff=lambda r: 3*r,\n",
    "                      )\n",
    "\n",
    "def regular_feature_type(gspace: gspaces.GSpace, planes: int, fixparams: bool = True):\n",
    "    \"\"\" build a regular feature map with the specified number of channels\"\"\"\n",
    "    assert gspace.fibergroup.order() > 0\n",
    "\n",
    "    N = gspace.fibergroup.order()\n",
    "\n",
    "    if fixparams:\n",
    "        planes *= math.sqrt(N)\n",
    "\n",
    "    planes = planes / N\n",
    "    planes = int(planes)\n",
    "\n",
    "    return enn.FieldType(gspace, [gspace.regular_repr] * planes)\n",
    "\n",
    "\n",
    "def trivial_feature_type(gspace: gspaces.GSpace, planes: int, fixparams: bool = True):\n",
    "    \"\"\" build a trivial feature map with the specified number of channels\"\"\"\n",
    "\n",
    "    if fixparams:\n",
    "        planes *= math.sqrt(gspace.fibergroup.order())\n",
    "\n",
    "    planes = int(planes)\n",
    "    return enn.FieldType(gspace, [gspace.trivial_repr] * planes)\n",
    "\n",
    "\n",
    "\n",
    "FIELD_TYPE = {\n",
    "    \"trivial\": trivial_feature_type,\n",
    "    \"regular\": regular_feature_type,\n",
    "}       \n",
    "\n",
    "class BasicBlock(enn.EquivariantModule):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_type: enn.FieldType,\n",
    "                 inner_type: enn.FieldType,\n",
    "                 dropout_rate: float,\n",
    "                 stride: int = 1,\n",
    "                 out_type: enn.FieldType = None,\n",
    "                 ):\n",
    "        super(BasicBlock, self).__init__()\n",
    "\n",
    "        if out_type is None:\n",
    "            out_type = in_type\n",
    "\n",
    "        self.in_type = in_type\n",
    "        inner_type = inner_type\n",
    "        self.out_type = out_type\n",
    "\n",
    "        conv = conv3x3\n",
    "\n",
    "        self.bn1 = enn.InnerBatchNorm(self.in_type)\n",
    "        self.relu1 = enn.ReLU(self.in_type, inplace=True)\n",
    "        self.conv1 = conv(self.in_type, inner_type)\n",
    "\n",
    "        self.bn2 = enn.InnerBatchNorm(inner_type)\n",
    "        self.relu2 = enn.ReLU(inner_type, inplace=True)\n",
    "\n",
    "        self.dropout = enn.PointwiseDropout(inner_type, p=dropout_rate)\n",
    "\n",
    "        self.conv2 = conv(inner_type, self.out_type, stride=stride)\n",
    "\n",
    "        self.shortcut = None\n",
    "        if stride != 1 or self.in_type != self.out_type:\n",
    "            self.shortcut = conv1x1(self.in_type, self.out_type, stride=stride, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_n = self.relu1(self.bn1(x))\n",
    "        out = self.relu2(self.bn2(self.conv1(x_n)))\n",
    "        out = self.dropout(out)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        if self.shortcut is not None:\n",
    "            out += self.shortcut(x_n)\n",
    "        else:\n",
    "            out += x\n",
    "\n",
    "        return out\n",
    "\n",
    "    def evaluate_output_shape(self, input_shape: Tuple):\n",
    "        assert len(input_shape) == 4\n",
    "        assert input_shape[1] == self.in_type.size\n",
    "        if self.shortcut is not None:\n",
    "            return self.shortcut.evaluate_output_shape(input_shape)\n",
    "        else:\n",
    "            return input_shape\n",
    "\n",
    "\n",
    "class ResNet18(torch.nn.Module):\n",
    "    def __init__(self, dropout_rate, num_classes=100,\n",
    "                 N: int = 4,\n",
    "                 r: int = 0,\n",
    "                 f: bool = False,\n",
    "                 deltaorth: bool = False,\n",
    "                 fixparams: bool = True,\n",
    "                 initial_stride: int = 1,\n",
    "                 ):\n",
    "        r\"\"\"\n",
    "        \n",
    "        Build and equivariant ResNet-18.\n",
    "        \n",
    "        The parameter ``N`` controls rotation equivariance and the parameter ``f`` reflection equivariance.\n",
    "        \n",
    "        More precisely, ``N`` is the number of discrete rotations the model is initially equivariant to.\n",
    "        ``N = 1`` means the model is only reflection equivariant from the beginning.\n",
    "        \n",
    "        ``f`` is a boolean flag specifying whether the model should be reflection equivariant or not.\n",
    "        If it is ``False``, the model is not reflection equivariant.\n",
    "        \n",
    "        ``r`` is the restriction level:\n",
    "        \n",
    "        - ``0``: no restriction. The model is equivariant to ``N`` rotations from the input to the output\n",
    "        - ``1``: restriction before the last block. The model is equivariant to ``N`` rotations before the last block\n",
    "               (i.e. in the first 2 blocks). Then it is restricted to ``N/2`` rotations until the output.\n",
    "        \n",
    "        - ``2``: restriction after the first block. The model is equivariant to ``N`` rotations in the first block.\n",
    "               Then it is restricted to ``N/2`` rotations until the output (i.e. in the last 3 blocks).\n",
    "               \n",
    "        - ``3``: restriction after the first and the second block. The model is equivariant to ``N`` rotations in the first\n",
    "               block. It is restricted to ``N/2`` rotations before the second block and to ``1`` rotations before the last\n",
    "               block.\n",
    "        \n",
    "        NOTICE: if restriction to ``N/2`` is performed, ``N`` needs to be even!\n",
    "        \n",
    "        \"\"\"\n",
    "        super(ResNet18, self).__init__()\n",
    "\n",
    "        nStages = [16, 16, 32, 64, 128]\n",
    "\n",
    "        self._fixparams = fixparams\n",
    "\n",
    "        self._layer = 0\n",
    "\n",
    "        # number of discrete rotations to be equivariant to\n",
    "        self._N = N\n",
    "\n",
    "        # if the model is [F]lip equivariant\n",
    "        self._f = f\n",
    "        if self._f:\n",
    "            if N != 1:\n",
    "                self.gspace = gspaces.FlipRot2dOnR2(N)\n",
    "            else:\n",
    "                self.gspace = gspaces.Flip2dOnR2()\n",
    "        else:\n",
    "            if N != 1:\n",
    "                self.gspace = gspaces.Rot2dOnR2(N)\n",
    "            else:\n",
    "                self.gspace = gspaces.TrivialOnR2()\n",
    "\n",
    "        # level of [R]estriction:\n",
    "        #   r = 0: never do restriction, i.e. initial group (either DN or CN) preserved for the whole network\n",
    "        #   r = 1: restrict before the last block, i.e. initial group (either DN or CN) preserved for the first\n",
    "        #          2 blocks, then restrict to N/2 rotations (either D{N/2} or C{N/2}) in the last block\n",
    "        #   r = 2: restrict after the first block, i.e. initial group (either DN or CN) preserved for the first\n",
    "        #          block, then restrict to N/2 rotations (either D{N/2} or C{N/2}) in the last 2 blocks\n",
    "        #   r = 3: restrict after each block. Initial group (either DN or CN) preserved for the first\n",
    "        #          block, then restrict to N/2 rotations (either D{N/2} or C{N/2}) in the second block and to 1 rotation\n",
    "        #          in the last one (D1 or C1)\n",
    "        assert r in [0, 1, 2, 3]\n",
    "        self._r = r\n",
    "\n",
    "        # the input has 3 color channels (RGB).\n",
    "        # Color channels are trivial fields and don't transform when the input is rotated or flipped\n",
    "        r1 = enn.FieldType(self.gspace, [self.gspace.trivial_repr] * 3)\n",
    "\n",
    "        # input field type of the model\n",
    "        self.in_type = r1\n",
    "\n",
    "        # in the first layer we always scale up the output channels to allow for enough independent filters\n",
    "        r2 = FIELD_TYPE[\"regular\"](self.gspace, nStages[0], fixparams=self._fixparams)\n",
    "\n",
    "        # dummy attribute keeping track of the output field type of the last submodule built, i.e. the input field type of\n",
    "        # the next submodule to build\n",
    "        self._in_type = r2\n",
    "\n",
    "        # Number of blocks per layer\n",
    "        n = 2\n",
    "\n",
    "        self.conv1 = conv3x3(r1, r2)\n",
    "        self.layer1 = self.basicLayer(BasicBlock, nStages[1], n, dropout_rate, stride=1)\n",
    "        self.layer2 = self.basicLayer(BasicBlock, nStages[2], n, dropout_rate, stride=2)\n",
    "        self.layer3 = self.basicLayer(BasicBlock, nStages[3], n, dropout_rate, stride=2)\n",
    "        # last layer maps to a trivial (invariant) feature map\n",
    "        self.layer4 = self.basicLayer(BasicBlock, nStages[4], n, dropout_rate, stride=2, totrivial=True)\n",
    "\n",
    "        self.bn = enn.InnerBatchNorm(self.layer4.out_type, momentum=0.9)\n",
    "        self.relu = enn.ReLU(self.bn.out_type, inplace=True)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.gpool = enn.GroupPooling(self.bn.out_type)\n",
    "        self.linear = torch.nn.Linear(self.gpool.out_type.size, num_classes)\n",
    "\n",
    "        for name, module in self.named_modules():\n",
    "            if isinstance(module, enn.R2Conv):\n",
    "                if deltaorth:\n",
    "                    init.deltaorthonormal_init(module.weights, module.basisexpansion)\n",
    "            elif isinstance(module, torch.nn.BatchNorm2d):\n",
    "                module.weight.data.fill_(1)\n",
    "                module.bias.data.zero_()\n",
    "            elif isinstance(module, torch.nn.Linear):\n",
    "                module.bias.data.zero_()\n",
    "\n",
    "        print(\"MODEL TOPOLOGY:\")\n",
    "        for i, (name, mod) in enumerate(self.named_modules()):\n",
    "            print(f\"\\t{i} - {name}\")\n",
    "\n",
    "    def basicLayer(self, block, planes: int, num_blocks: int, dropout_rate: float, stride: int,\n",
    "                    totrivial: bool = False\n",
    "                    ) -> enn.SequentialModule:\n",
    "\n",
    "        self._layer += 1\n",
    "        print(\"start building\", self._layer)\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "\n",
    "        main_type = FIELD_TYPE[\"regular\"](self.gspace, planes, fixparams=self._fixparams)\n",
    "        inner_type = FIELD_TYPE[\"regular\"](self.gspace, planes, fixparams=self._fixparams)\n",
    "\n",
    "        if totrivial:\n",
    "            out_type = FIELD_TYPE[\"trivial\"](self.gspace, planes, fixparams=self._fixparams)\n",
    "        else:\n",
    "            out_type = FIELD_TYPE[\"regular\"](self.gspace, planes, fixparams=self._fixparams)\n",
    "\n",
    "        for b, stride in enumerate(strides):\n",
    "            if b == num_blocks - 1:\n",
    "                out_f = out_type\n",
    "            else:\n",
    "                out_f = main_type\n",
    "            layers.append(block(self._in_type, inner_type, dropout_rate, stride, out_type=out_f))\n",
    "            self._in_type = out_f\n",
    "\n",
    "        print(\"layer\", self._layer, \"built\")\n",
    "        return enn.SequentialModule(*layers)\n",
    "\n",
    "    def features(self, x):\n",
    "\n",
    "        x = enn.GeometricTensor(x, self.in_type)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "\n",
    "        x1 = self.layer1(out)\n",
    "\n",
    "        x2 = self.layer2(x1)\n",
    "\n",
    "        x3 = self.layer3(x2)\n",
    "\n",
    "        x4 = self.layer4(x3)\n",
    "\n",
    "        return x1, x2, x3, x4\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # wrap the input tensor in a GeometricTensor\n",
    "        x = enn.GeometricTensor(x, self.in_type)\n",
    "        out = self.conv1(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.gpool(out)\n",
    "\n",
    "        # extract the tensor from the GeometricTensor to use the common Pytorch operations\n",
    "        out = out.tensor\n",
    "        gpool_out = out\n",
    "\n",
    "        b, c, w, h = out.shape\n",
    "        out = F.avg_pool2d(out, (w, h))\n",
    "\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out, gpool_out"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
