{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDSS Astroquery Batch\n",
    "\n",
    "Batch runs through the images used in the GZ1 (SDSS DR7) dataset and returns half-light radii etc for data cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import astropy.units as u\n",
    "from astroquery.sdss import SDSS\n",
    "from astropy.coordinates import SkyCoord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATALOG_PATH = '../Data/gz1_desi_cross_cat.csv'\n",
    "OUTPUT_PATH = '../Data/gz1_desi_cross_cat_queried.csv'\n",
    "RADIUS = \"1 arcsec\"\n",
    "catalog = pd.read_csv(CATALOG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Field info are not available for this data release [astroquery.sdss.field_names]\n",
      "WARNING: Field info are not available for this data release [astroquery.sdss.field_names]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 0 (3240 items, 6071 results found))\n",
      "Processing batch 0 (3240 items, 6071 results found, cut to 1342))\n"
     ]
    }
   ],
   "source": [
    "def split_dataframe(data, no_of_batches):\n",
    "    batch_size = math.ceil(data.shape[0] / no_of_batches)\n",
    "    batched_df = [data[i:i+batch_size] for i in range(0,data.shape[0], batch_size)]\n",
    "    return batched_df\n",
    "\n",
    "def get_SDSS_info_batch():\n",
    "    batched_df = split_dataframe(catalog,200) #30s per batch, more than this seems to fail\n",
    "\n",
    "    if os.path.exists(OUTPUT_PATH):\n",
    "        os.remove(OUTPUT_PATH)\n",
    "\n",
    "    for i, batch in enumerate(batched_df):\n",
    "        #print(f\"Processing batch {i}\")\n",
    "        \n",
    "        coords = SkyCoord(batch[\"RA\"],batch[\"DEC\"],unit=(u.hourangle, u.deg))\n",
    "        results = pd.DataFrame(SDSS.query_region(coords,data_release=7,radius=RADIUS,photoobj_fields=[\"objID\",\"ra\",\"dec\",\"err_r\",\"petroR50_r\",\"petroR50Err_r\"]).to_pandas())\n",
    "    \n",
    "        #Clean up OBJID fields\n",
    "        batch.loc[:,'OBJID'] = batch['OBJID'].astype(str).str.strip()\n",
    "        results.loc[:,'objID'] = results['objID'].astype(str).str.strip()\n",
    "        \n",
    "        k=0\n",
    "        j=0\n",
    "        rows_list = []\n",
    "        while k < len(batch)-1: #Run through each item in batch\n",
    "            batch_row = batch.iloc[k]\n",
    "            results_row = results.iloc[j]\n",
    "            \n",
    "            if batch_row['OBJID'] == results_row['objID']: #If OBJIDs match\n",
    "                #print(f\"Match at row {k}\")\n",
    "                if batch.iloc[k+1]['OBJID'] == results.iloc[j+1]['objID']: #If next object OBJIDs match\n",
    "                    #print(f\"Adding row {k} as next row matches\")\n",
    "                    batch_dict = batch_row.to_dict()\n",
    "                    results_dict = results_row.to_dict()\n",
    "                    batch_dict.update(results_dict)# Add matching rows from batch and results\n",
    "                    rows_list.append(batch_dict)\n",
    "                else:\n",
    "                    #print(f\"Skipping row {k} as next row does not match\")\n",
    "                    while batch.iloc[k+1]['OBJID'] != results.iloc[j+1]['objID']:\n",
    "                        j += 1 # Move through results until match found\n",
    "            k += 1 #Move on to next i\n",
    "            j += 1 # Move on to next j\n",
    "\n",
    "        final_columns = batch.columns.to_list()+results.columns.to_list()\n",
    "        final = pd.DataFrame(rows_list,columns= final_columns)\n",
    "        reduced = final.drop([\"Unnamed: 0\",\"objID\",\"ra\",\"dec\"],axis=1)\n",
    "        #print(f\"Length of final: {len(final)}, with {len(pd.unique(final['OBJID']))} unique\")\n",
    "        \n",
    "        time.sleep(1)\n",
    "        reduced.to_csv(OUTPUT_PATH, mode='a', header=not os.path.exists(OUTPUT_PATH),index=False)\n",
    "        print(f\"Processing batch {i} ({len(batch)} items, {len(results)} results found, cut to {len(reduced)}))\")\n",
    "\n",
    "get_SDSS_info_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #reduced =  catalog.drop_duplicates(subset=['OBJID'])\n",
    "# reduced = catalog.drop([\"Unnamed: 0\",\"Unnamed: 0.1\"],axis=1)\n",
    "# print(reduced.shape[0])\n",
    "# print(f\"Number of galaxies in GZ1 catalogue: {catalog.shape[0]}\")\n",
    "# print(f\"Columns: {catalog.columns.values}\")\n",
    "# reduced.head(10)\n",
    "# #reduced.to_csv(OUTPUT_PATH,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mphys-galaxy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
