{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch import optim, nn, utils, Tensor\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "import lightning as L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define any number of nn.Modules (or use your current ones)\n",
    "encoder = nn.Sequential(nn.Linear(28 * 28, 64), nn.ReLU(), nn.Linear(64, 3))\n",
    "decoder = nn.Sequential(nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 28 * 28))\n",
    "\n",
    "\n",
    "# define the LightningModule\n",
    "class LitAutoEncoder(L.LightningModule):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        # it is independent of forward\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        loss = nn.functional.mse_loss(x_hat, x)\n",
    "        # Logging to TensorBoard (if installed) by default\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "# init the autoencoder\n",
    "autoencoder = LitAutoEncoder(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from argparse import ArgumentParser\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import Accuracy\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "\n",
    "class ResNetClassifier(pl.LightningModule):\n",
    "    resnets = {\n",
    "        18: models.resnet18,\n",
    "        34: models.resnet34,\n",
    "        50: models.resnet50,\n",
    "        101: models.resnet101,\n",
    "        152: models.resnet152,\n",
    "    }\n",
    "    optimizers = {\"adam\": Adam, \"sgd\": SGD}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes,\n",
    "        resnet_version,\n",
    "        train_path,\n",
    "        val_path,\n",
    "        test_path=None,\n",
    "        optimizer=\"adam\",\n",
    "        lr=1e-3,\n",
    "        batch_size=16,\n",
    "        transfer=True,\n",
    "        tune_fc_only=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.train_path = train_path\n",
    "        self.val_path = val_path\n",
    "        self.test_path = test_path\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.optimizer = self.optimizers[optimizer]\n",
    "        # instantiate loss criterion\n",
    "        self.loss_fn = (\n",
    "            nn.BCEWithLogitsLoss() if num_classes == 1 else nn.CrossEntropyLoss()\n",
    "        )\n",
    "        # create accuracy metric\n",
    "        self.acc = Accuracy(\n",
    "            task=\"binary\" if num_classes == 1 else \"multiclass\", num_classes=num_classes\n",
    "        )\n",
    "        # Using a pretrained ResNet backbone\n",
    "        self.resnet_model = self.resnets[resnet_version](pretrained=transfer)\n",
    "        # Replace old FC layer with Identity so we can train our own\n",
    "        linear_size = list(self.resnet_model.children())[-1].in_features\n",
    "        # replace final layer for fine tuning\n",
    "        self.resnet_model.fc = nn.Linear(linear_size, num_classes)\n",
    "\n",
    "        if tune_fc_only:  # option to only tune the fully-connected layers\n",
    "            for child in list(self.resnet_model.children())[:-1]:\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.resnet_model(X)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.optimizer(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def _step(self, batch):\n",
    "        x, y = batch\n",
    "        preds = self(x)\n",
    "\n",
    "        if self.num_classes == 1:\n",
    "            preds = preds.flatten()\n",
    "            y = y.float()\n",
    "\n",
    "        loss = self.loss_fn(preds, y)\n",
    "        acc = self.acc(preds, y)\n",
    "        return loss, acc\n",
    "\n",
    "    def _dataloader(self, data_path, shuffle=False):\n",
    "        # values here are specific to pneumonia dataset and should be updated for custom data\n",
    "        transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((500, 500)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.48232,), (0.23051,)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        img_folder = ImageFolder(data_path, transform=transform)\n",
    "\n",
    "        return DataLoader(img_folder, batch_size=self.batch_size, shuffle=shuffle)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self._dataloader(self.train_path, shuffle=True)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, acc = self._step(batch)\n",
    "        # perform logging\n",
    "        self.log(\n",
    "            \"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "        self.log(\n",
    "            \"train_acc\", acc, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self._dataloader(self.val_path)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, acc = self._step(batch)\n",
    "        # perform logging\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=False, logger=True)\n",
    "        self.log(\"val_acc\", acc, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self._dataloader(self.test_path)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, acc = self._step(batch)\n",
    "        # perform logging\n",
    "        self.log(\"test_loss\", loss, on_step=True, prog_bar=True, logger=True)\n",
    "        self.log(\"test_acc\", acc, on_step=True, prog_bar=True, logger=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = ArgumentParser()\n",
    "    # Required arguments\n",
    "    parser.add_argument(\n",
    "        \"model\",\n",
    "        help=\"\"\"Choose one of the predefined ResNet models provided by torchvision. e.g. 50\"\"\",\n",
    "        type=int,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"num_classes\", help=\"\"\"Number of classes to be learned.\"\"\", type=int\n",
    "    )\n",
    "    parser.add_argument(\"num_epochs\", help=\"\"\"Number of Epochs to Run.\"\"\", type=int)\n",
    "    parser.add_argument(\n",
    "        \"train_set\", help=\"\"\"Path to training data folder.\"\"\", type=Path\n",
    "    )\n",
    "    parser.add_argument(\"val_set\", help=\"\"\"Path to validation set folder.\"\"\", type=Path)\n",
    "    # Optional arguments\n",
    "    parser.add_argument(\n",
    "        \"-amp\",\n",
    "        \"--mixed_precision\",\n",
    "        help=\"\"\"Use mixed precision during training. Defaults to False.\"\"\",\n",
    "        action=\"store_true\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-ts\", \"--test_set\", help=\"\"\"Optional test set path.\"\"\", type=Path\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-o\",\n",
    "        \"--optimizer\",\n",
    "        help=\"\"\"PyTorch optimizer to use. Defaults to adam.\"\"\",\n",
    "        default=\"adam\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-lr\",\n",
    "        \"--learning_rate\",\n",
    "        help=\"Adjust learning rate of optimizer.\",\n",
    "        type=float,\n",
    "        default=1e-3,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-b\",\n",
    "        \"--batch_size\",\n",
    "        help=\"\"\"Manually determine batch size. Defaults to 16.\"\"\",\n",
    "        type=int,\n",
    "        default=16,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-tr\",\n",
    "        \"--transfer\",\n",
    "        help=\"\"\"Determine whether to use pretrained model or train from scratch. Defaults to True.\"\"\",\n",
    "        action=\"store_true\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-to\",\n",
    "        \"--tune_fc_only\",\n",
    "        help=\"Tune only the final, fully connected layers.\",\n",
    "        action=\"store_true\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-s\", \"--save_path\", help=\"\"\"Path to save model trained model checkpoint.\"\"\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-g\", \"--gpus\", help=\"\"\"Enables GPU acceleration.\"\"\", type=int, default=None\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # # Instantiate Model\n",
    "    model = ResNetClassifier(\n",
    "        num_classes=args.num_classes,\n",
    "        resnet_version=args.model,\n",
    "        train_path=args.train_set,\n",
    "        val_path=args.val_set,\n",
    "        test_path=args.test_set,\n",
    "        optimizer=args.optimizer,\n",
    "        lr=args.learning_rate,\n",
    "        batch_size=args.batch_size,\n",
    "        transfer=args.transfer,\n",
    "        tune_fc_only=args.tune_fc_only,\n",
    "    )\n",
    "\n",
    "    save_path = args.save_path if args.save_path is not None else \"./models\"\n",
    "    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "        dirpath=save_path,\n",
    "        filename=\"resnet-model-{epoch}-{val_loss:.2f}-{val_acc:0.2f}\",\n",
    "        monitor=\"val_loss\",\n",
    "        save_top_k=3,\n",
    "        mode=\"min\",\n",
    "        save_last=True,\n",
    "    )\n",
    "\n",
    "    stopping_callback = pl.callbacks.EarlyStopping()\n",
    "\n",
    "    # Instantiate lightning trainer and train model\n",
    "    trainer_args = {\n",
    "        \"accelerator\": \"gpu\" if args.gpus else None,\n",
    "        \"devices\": [1],\n",
    "        \"strategy\": \"dp\" if args.gpus > 1 else None,\n",
    "        \"max_epochs\": args.num_epochs,\n",
    "        \"callbacks\": [checkpoint_callback],\n",
    "        \"precision\": 16 if args.mixed_precision else 32,\n",
    "    }\n",
    "    trainer = pl.Trainer(**trainer_args)\n",
    "\n",
    "    trainer.fit(model)\n",
    "\n",
    "    if args.test_set:\n",
    "        trainer.test(model)\n",
    "    # Save trained model weights\n",
    "    #torch.save(trainer.model.resnet_model.state_dict(), save_path + \"/trained_model.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mphys-galaxy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
