{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Lightning Framework for training S+Z Galaxy Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from enum import Enum\n",
    "import torch\n",
    "import lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger,CSVLogger\n",
    "\n",
    "from ChiralityClassifier import ChiralityClassifier\n",
    "from dataset_utils import *\n",
    "from metrics_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class datasets(Enum):\n",
    "    FULL_DATASET = 0 #Use all 600,000 galaxies in GZ1 catalog\n",
    "    CUT_DATASET = 1 #Use cut of 200,000 galaxies, with pre-selected test data and downsampled train data\n",
    "    BEST_SUBSET = 2 #Select N best S,Z & other galaxies, evenly split\n",
    "    LOCAL_SUBSET = 3 #Use local cache of 1500 galaxies\n",
    "    FULL_DESI_DATASET = 4 #Use all 7 million galaxies in DESI catalog, minus those that appear in cut catalog (predict only)\n",
    "    CUT_TEST_DATASET = 5 #The testing dataset from the CUT_DATASET (predict only)\n",
    "\n",
    "class modes(Enum):\n",
    "    TRAIN = 0 #Train on a dataset\n",
    "    TEST = 1 #Test an existing saved model on a labelled dataset\n",
    "    PREDICT = 2 #Use an existing saved model on an labelled/unlabelled dataset\n",
    "\n",
    "DATASET = datasets.LOCAL_SUBSET #Select which dataset to train on, or if testing/predicting, which dataset the model was trained on\n",
    "MODE = modes.TEST #Select which mode\n",
    "\n",
    "PREDICT_DATASET = datasets.LOCAL_SUBSET #If predicting, predict this dataset\n",
    "SET_CHIRALITY = None #Set to None unless you want to use galaxies from the CUT_DATASET's test dataset with only S and Z galaxies at a set chirality violation (predict only)\n",
    "\n",
    "# Models:\n",
    "#resnet18,resnet34,resnet50,resnet101,resnet152,\n",
    "#ce_resnet50,lenet,g_resnet18,g_resnet50,g_lenet,g_resnet18_old\n",
    "MODEL_NAME = \"resnet18\"\n",
    "CUSTOM_ID = \"nr\"\n",
    "\n",
    "USE_TENSORBOARD = True #Log to tensorboard as well as csv logger\n",
    "SAVE_MODEL = True #Save model weights to .pt file\n",
    "REPEAT_RUNS = [0] #Set to [0] for 1 run, or a list for specific runs\n",
    "IMG_SIZE = 160 #This is the output size of the generated image array\n",
    "NUM_WORKERS = 6 #Number of workers in dataloader (usually set to no of CPU cores - 1)\n",
    "MAX_IMAGES = -1 #Max number of images to load (-1 for all)\n",
    "FLIP_EQUIVARIANCE = False #Enable flip-equivariance (g_resnet models only)\n",
    "CUSTOM_PREDICT = False #Use Jia et al (2023) flipped predict function (g_resnet models only)\n",
    "RANDOM_ROTATE = False #Randomly rotate images between 0-360 degrees in training/testing\n",
    "ENABLE_DROPOUT = False #Add dropout layer (g_resnet and ce-resnet models only)\n",
    "\n",
    "#HYPERPARAMS\n",
    "BATCH_SIZE = 100 #Number of images per batch, cannot be >60 for resnet50_c (takes 45GB ram)\n",
    "LEARNING_RATE = 0.0001\n",
    "MAX_EPOCHS = 60\n",
    "\n",
    "PATHS = dict(\n",
    "    METRICS_PATH = \"../Metrics\",\n",
    "    LOG_PATH = \"../Code/lightning_logs\",\n",
    "    FULL_DATA_PATH =  \"/share/nas2/walml/galaxy_zoo/decals/dr8/jpg\",\n",
    "    LOCAL_SUBSET_DATA_PATH =  \"../Data/Subset\",\n",
    "    FULL_CATALOG_PATH =  \"../Data/gz1_desi_cross_cat.csv\",\n",
    "    FULL_DESI_CATALOG_PATH =  \"../Data/desi_full_cat.parquet\",\n",
    "    CUT_CATALOG_TEST_PATH =  \"../Data/gz1_desi_cross_cat_testing.csv\",\n",
    "    CUT_CATALOG_TRAIN_PATH = \"../Data/gz1_desi_cross_cat_train_val_downsample.csv\",\n",
    "    BEST_SUBSET_CATALOG_PATH =  \"../Data/gz1_desi_cross_cat_best_subset.csv\",\n",
    "    LOCAL_SUBSET_CATALOG_PATH =  \"../Data/gz1_desi_cross_cat_local_subset.csv\",\n",
    ")\n",
    "\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "if len(CUSTOM_ID) == 0:\n",
    "    MODEL_ID = f\"{MODEL_NAME}_{DATASET.name.lower()}\"\n",
    "else:\n",
    "     MODEL_ID = f\"{MODEL_NAME}_{DATASET.name.lower()}_{CUSTOM_ID}\"\n",
    "print(f\"********** Running model {MODEL_ID} in mode {MODE.name.lower()} for runs {REPEAT_RUNS} ***********\")\n",
    "if MODE != modes.TRAIN:\n",
    "    USE_TENSORBOARD = False #Don\"t log to tensorboard if not training\n",
    "    SAVE_MODEL = False #Don\"t save weights if testing or predicting model\n",
    "    RANDOM_ROTATE = False #Do not add random rotations in testing or predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE == modes.PREDICT:\n",
    "    datamodule = generate_datamodule(PREDICT_DATASET,MODE,PATHS,datasets,modes,IMG_SIZE,BATCH_SIZE,NUM_WORKERS,MAX_IMAGES,RANDOM_ROTATE,SET_CHIRALITY)\n",
    "    datamodule.prepare_data()\n",
    "    datamodule.setup(stage='predict')\n",
    "else:\n",
    "    datamodule = generate_datamodule(DATASET,MODE,PATHS,datasets,modes,IMG_SIZE,BATCH_SIZE,NUM_WORKERS,MAX_IMAGES,RANDOM_ROTATE)\n",
    "    datamodule.prepare_data()\n",
    "    datamodule.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in range(0,REPEAT_RUNS):\n",
    "    print(f\"*************************************** Run number {run} ***************************************\")\n",
    "   \n",
    "    save_dir = f\"{PATHS['METRICS_PATH']}/{MODEL_ID}/version_{run}\"\n",
    "    MODEL_PATH = f\"{save_dir}/model.pt\"\n",
    "    create_folder(save_dir)\n",
    "\n",
    "    model = ChiralityClassifier(\n",
    "        num_classes=(2 if (MODEL_NAME==\"ce_resnet50\" or CUSTOM_PREDICT) else 3), #2 for Jia et al version\n",
    "        model_version=MODEL_NAME,\n",
    "        optimizer=\"adamw\",\n",
    "        scheduler  =\"steplr\",\n",
    "        lr=LEARNING_RATE,\n",
    "        weight_decay=0,\n",
    "        step_size=5,\n",
    "        gamma=0.85,\n",
    "        weights=(MODEL_PATH if MODE != modes.TRAIN else None),\n",
    "        graph_save_path=(f\"{save_dir}/val_matrix.png\" if MODE == modes.TRAIN else f\"{save_dir}/{MODE.name.lower()}_matrix.png\"),\n",
    "        flip_eq=FLIP_EQUIVARIANCE,\n",
    "        custom_predict = CUSTOM_PREDICT,\n",
    "        enable_dropout = ENABLE_DROPOUT\n",
    "    )\n",
    "\n",
    "    tb_logger = TensorBoardLogger(PATHS[\"LOG_PATH\"], name=MODEL_ID,version=f\"version_{run}_{MODE.name.lower()}\")\n",
    "    csv_logger = CSVLogger(PATHS[\"LOG_PATH\"],name=MODEL_ID,version=f\"version_{run}_{MODE.name.lower()}\")\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=(\"gpu\" if device.type==\"cuda\" else \"cpu\"),\n",
    "        max_epochs=MAX_EPOCHS,\n",
    "        devices=1,\n",
    "        logger=([tb_logger,csv_logger] if USE_TENSORBOARD else csv_logger),\n",
    "        default_root_dir=f\"{PATHS['LOG_PATH']}/{MODEL_ID}\",\n",
    "        enable_checkpointing=False,\n",
    "    )\n",
    "    \n",
    "    if MODE==modes.TRAIN:\n",
    "        trainer.fit(model,train_dataloaders=datamodule.train_dataloader(),val_dataloaders=datamodule.val_dataloader())\n",
    "        trainer.test(model,dataloaders=datamodule.val_dataloader())\n",
    "\n",
    "        if SAVE_MODEL:\n",
    "            torch.save(trainer.model.state_dict(), MODEL_PATH)\n",
    "        \n",
    "    elif MODE==modes.TEST:\n",
    "        trainer.test(model,dataloaders=datamodule.test_dataloader())\n",
    "        \n",
    "    elif MODE==modes.PREDICT:\n",
    "\n",
    "        if PREDICT_DATASET == datasets.FULL_DESI_DATASET:\n",
    "            #Need to split into smaller batches to avoid keeping an array of 8.7mil predictions\n",
    "            dataloader_len = len(datamodule.predict_dataset)\n",
    "            total_predict_batches = min(10, dataloader_len)\n",
    "            subset_size = dataloader_len // total_predict_batches\n",
    "            for i in range(total_predict_batches):\n",
    "                print(f\"Loading predict batch {i} (size {subset_size}) of {total_predict_batches} (size {dataloader_len})\")\n",
    "                start_idx = i * subset_size\n",
    "                end_idx = min((i + 1) * subset_size, dataloader_len)\n",
    "\n",
    "                #Create a new dataloader only containing the subset of objects\n",
    "                subset = torch.utils.data.Subset(datamodule.predict_dataset,range(start_idx,end_idx))\n",
    "                subset_loader = pl.LightningDataModule().from_datasets(predict_dataset=subset,batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "                subset_loader.prepare_data()\n",
    "                subset_loader.setup(stage='predict')\n",
    "                predictions = trainer.predict(model,dataloaders=subset_loader.predict_dataloader())\n",
    "                predict_save_path = f\"{save_dir}/{PREDICT_DATASET.name.lower()}_{i}_predictions.csv\"\n",
    "                if os.path.exists(predict_save_path):\n",
    "                    os.remove(predict_save_path)\n",
    "                # ordered by batches\n",
    "                for batch in predictions: #Save predictions\n",
    "                    batch = pd.DataFrame(torch.softmax(batch,dim=1))\n",
    "                    batch.to_csv(predict_save_path,mode='a', index=False, header=False)\n",
    "        else: \n",
    "            if SET_CHIRALITY is not None:\n",
    "                predict_save_path = f\"{save_dir}/{PREDICT_DATASET.name.lower()}_CVIOL_{SET_CHIRALITY}_predictions.csv\"\n",
    "            else:\n",
    "                predict_save_path = f\"{save_dir}/{PREDICT_DATASET.name.lower()}_predictions.csv\"\n",
    "            if os.path.exists(predict_save_path):\n",
    "                os.remove(predict_save_path)\n",
    "            predictions = trainer.predict(model,dataloaders=datamodule.predict_dataloader())\n",
    "            for batch in predictions: #Save predictions\n",
    "                batch = pd.DataFrame(torch.softmax(batch,dim=1))\n",
    "                batch.to_csv(predict_save_path,mode='a', index=False, header=False)\n",
    "\n",
    "    if MODE != modes.PREDICT and MODE != modes.SHAP and SET_CHIRALITY is None:\n",
    "        #Save cleaned up logs file to Metrics folder & save graph\n",
    "        save_metrics_from_logger(MODEL_ID,PATHS[\"LOG_PATH\"],PATHS['METRICS_PATH'],version=run,mode=MODE.name.lower(),save=True)  \n",
    "        if MODE==modes.TRAIN:\n",
    "            plot_train_metrics(MODEL_ID,PATHS['METRICS_PATH'],version=run,show=False,save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dereference all objects, clear cuda cache and run garbage collection\n",
    "datamodule=None\n",
    "model=None\n",
    "trainer=None\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mphys-galaxy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
