{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Lightning Framework for training S+Z Galaxy Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GZDESI/GZRings/GZCD not available from galaxy_datasets.pytorch.datasets - skipping\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from enum import Enum\n",
    "import torch\n",
    "import lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger,CSVLogger\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "from ChiralityClassifier import ChiralityClassifier\n",
    "from dataset_utils import *\n",
    "from metrics_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class datasets(Enum):\n",
    "    FULL_DATASET = 0 #Use all 600,000 galaxies in GZ1 catalog\n",
    "    CUT_DATASET = 1 #Use cut of 200,000 galaxies, with pre-selected test data and downsampled train data\n",
    "    BEST_SUBSET = 2 #Select N best S,Z & other galaxies, evenly split\n",
    "    LOCAL_SUBSET = 3 #Use local cache of 1500 galaxies\n",
    "    FULL_DESI_DATASET = 4 #Use all 7 million galaxies in DESI catalog, minus those that appear in cut catalog (predict only)\n",
    "\n",
    "class modes(Enum):\n",
    "    TRAIN = 0 #Train on a dataset\n",
    "    TEST = 1 #Test an existing saved model on a dataset\n",
    "    PREDICT = 2 #Use an existing saved model on an unlabelled dataset\n",
    "\n",
    "DATASET = datasets.CUT_DATASET #Select which dataset to train on, or if testing/predicting, which dataset the model was trained on\n",
    "MODE = modes.PREDICT #Select which mode\n",
    "\n",
    "PREDICT_DATASET = datasets.FULL_DESI_DATASET #If predicting, predict this dataset\n",
    "\n",
    "# Models:\n",
    "#resnet18,resnet34,resnet50,resnet101,resnet152,\n",
    "#jiaresnet50,lenet,g_resnet18,g_lenet,\n",
    "MODEL_NAME = \"resnet18\"\n",
    "CUSTOM_ID = \"\"\n",
    "\n",
    "USE_TENSORBOARD = False #Log to tensorboard as well as csv logger\n",
    "SAVE_MODEL = False #Save model weights to .pt file\n",
    "REPEAT_RUNS = 1 #Set to 1 for 1 run\n",
    "IMG_SIZE = 160 #This is the output size of the generated image array\n",
    "NUM_WORKERS = 11 #Number of workers in dataloader (usually set to no of CPU cores - 1)\n",
    "\n",
    "#HYPERPARAMS\n",
    "BATCH_SIZE = 100 #Number of images per batch\n",
    "LEARNING_RATE = 0.0001\n",
    "MAX_EPOCHS = 60\n",
    "\n",
    "PATHS = dict(\n",
    "    METRICS_PATH = \"../Metrics\",\n",
    "    LOG_PATH = \"../Code/lightning_logs\",\n",
    "    FULL_DATA_PATH =  \"/share/nas2/walml/galaxy_zoo/decals/dr8/jpg\",\n",
    "    LOCAL_SUBSET_DATA_PATH =  \"../Data/Subset\",\n",
    "    FULL_CATALOG_PATH =  \"../Data/gz1_desi_cross_cat.csv\",\n",
    "    FULL_DESI_CATALOG_PATH =  \"../Data/desi_full_cat.parquet\",\n",
    "    CUT_CATALOG_TEST_PATH =  \"../Data/gz1_desi_cross_cat_testing.csv\",\n",
    "    CUT_CATALOG_TRAIN_PATH = \"../Data/gz1_desi_cross_cat_train_val_downsample.csv\",\n",
    "    BEST_SUBSET_CATALOG_PATH =  \"../Data/gz1_desi_cross_cat_best_subset.csv\",\n",
    "    LOCAL_SUBSET_CATALOG_PATH =  \"../Data/gz1_desi_cross_cat_local_subset.csv\",\n",
    ")\n",
    "\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "if len(CUSTOM_ID) == 0:\n",
    "    MODEL_ID = f\"{MODEL_NAME}_{DATASET.name.lower()}\"\n",
    "else:\n",
    "     MODEL_ID = f\"{MODEL_NAME}_{DATASET.name.lower()}_{CUSTOM_ID}\"\n",
    "     \n",
    "if MODE != modes.TRAIN:\n",
    "    USE_TENSORBOARD = False #Don\"t log to tensorboard if not training\n",
    "    SAVE_MODEL = False #Don\"t save weights if testing or predicting model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pytorch 2.2.1. CPU cores available on device: 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA A100-PCIE-40GB\n",
      "Allocated Memory: 0.0 GB\n",
      "Cached Memory: 0.0 GB\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x8e in position 7: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m MODE \u001b[38;5;241m==\u001b[39m modes\u001b[38;5;241m.\u001b[39mPREDICT:\n\u001b[0;32m----> 2\u001b[0m     datamodule \u001b[38;5;241m=\u001b[39m generate_datamodule(PREDICT_DATASET,MODE,PATHS,datasets,modes,IMG_SIZE,BATCH_SIZE,NUM_WORKERS)\n\u001b[1;32m      3\u001b[0m     datamodule\u001b[38;5;241m.\u001b[39mprepare_data()\n\u001b[1;32m      4\u001b[0m     datamodule\u001b[38;5;241m.\u001b[39msetup(stage\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/share/nas2/npower/mphys-galaxy/Code/dataset_utils.py:79\u001b[0m, in \u001b[0;36mgenerate_datamodule\u001b[0;34m(DATASET, MODE, PATHS, datasets, modes, IMG_SIZE, BATCH_SIZE, NUM_WORKERS)\u001b[0m\n\u001b[1;32m     76\u001b[0m     catalog[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_loc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m get_file_paths(catalog,PATHS[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFULL_DATA_PATH\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DATASET \u001b[38;5;241m==\u001b[39m datasets\u001b[38;5;241m.\u001b[39mFULL_DESI_DATASET:\n\u001b[0;32m---> 79\u001b[0m     catalog \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(PATHS[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFULL_DESI_CATALOG_PATH\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     80\u001b[0m     catalog[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_loc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m get_file_paths(catalog,PATHS[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFULL_DATA_PATH\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m DATASET \u001b[38;5;241m==\u001b[39m datasets\u001b[38;5;241m.\u001b[39mBEST_SUBSET:\n",
      "File \u001b[0;32m/share/nas2/npower/miniconda3/envs/mphys-galaxy/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/share/nas2/npower/miniconda3/envs/mphys-galaxy/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/share/nas2/npower/miniconda3/envs/mphys-galaxy/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/share/nas2/npower/miniconda3/envs/mphys-galaxy/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1723\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1720\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1722\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[1;32m   1724\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1725\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/share/nas2/npower/miniconda3/envs/mphys-galaxy/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m parsers\u001b[38;5;241m.\u001b[39mTextReader(src, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[0;32mparsers.pyx:579\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:668\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:879\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2050\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x8e in position 7: invalid start byte"
     ]
    }
   ],
   "source": [
    "if MODE == modes.PREDICT:\n",
    "    datamodule = generate_datamodule(PREDICT_DATASET,MODE,PATHS,datasets,modes,IMG_SIZE,BATCH_SIZE,NUM_WORKERS)\n",
    "    datamodule.prepare_data()\n",
    "    datamodule.setup(stage='predict')\n",
    "else:\n",
    "    datamodule = generate_datamodule(DATASET,MODE,PATHS,datasets,modes,IMG_SIZE,BATCH_SIZE,NUM_WORKERS)\n",
    "    datamodule.prepare_data()\n",
    "    datamodule.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/nas2/npower/miniconda3/envs/mphys-galaxy/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:168: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /share/nas2/npower/miniconda3/envs/mphys-galaxy/lib/ ...\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/share/nas2/npower/miniconda3/envs/mphys-galaxy/lib/python3.11/site-packages/lightning_fabric/loggers/csv_logs.py:186: UserWarning: Experiment logs directory ../Code/lightning_logs/resnet18_best_subset/version_0_predict exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 150/150 [00:24<00:00,  6.19it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27241/2358250748.py:52: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  pd.DataFrame(np.array(predictions,dtype=object)).to_csv(f\"{save_dir}/{PREDICT_DATASET}_predictions.csv\")\n"
     ]
    }
   ],
   "source": [
    "for run in range(0,REPEAT_RUNS):\n",
    "    \n",
    "    save_dir = f\"{PATHS['METRICS_PATH']}/{MODEL_ID}/version_{run}\"\n",
    "    MODEL_PATH = f\"{save_dir}/model.pt\"\n",
    "    create_folder(save_dir)\n",
    "\n",
    "    model = ChiralityClassifier(\n",
    "        num_classes=(2 if (MODEL_NAME==\"jiaresnet50\") else 3), #2 for Jia et al version\n",
    "        model_version=MODEL_NAME,\n",
    "        optimizer=\"adamw\",\n",
    "        scheduler  =\"steplr\",\n",
    "        lr=LEARNING_RATE,\n",
    "        weight_decay=0,\n",
    "        step_size=5,\n",
    "        gamma=0.85,\n",
    "        weights=(MODEL_PATH if MODE != modes.TRAIN else None),\n",
    "        graph_save_path=(f\"{save_dir}/val_matrix.png\" if MODE == modes.TRAIN else f\"{save_dir}/{MODE.name.lower()}_matrix.png\")\n",
    "    )\n",
    "\n",
    "    tb_logger = TensorBoardLogger(PATHS[\"LOG_PATH\"], name=MODEL_ID,version=f\"version_{run}_{MODE.name.lower()}\")\n",
    "    csv_logger = CSVLogger(PATHS[\"LOG_PATH\"],name=MODEL_ID,version=f\"version_{run}_{MODE.name.lower()}\")\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=(\"gpu\" if device.type==\"cuda\" else \"cpu\"),\n",
    "        max_epochs=MAX_EPOCHS,\n",
    "        devices=1,\n",
    "        logger=([tb_logger,csv_logger] if USE_TENSORBOARD else csv_logger),\n",
    "        default_root_dir=f\"{PATHS['LOG_PATH']}/{MODEL_ID}\",\n",
    "        enable_checkpointing=False,\n",
    "        #profiler=\"pytorch\"\n",
    "        #callbacks=EarlyStopping(monitor=\"val_loss\", mode=\"min\")\n",
    "    )\n",
    "\n",
    "    #compiled_model = torch.compile(model, backend=\"eager\")\n",
    "    \n",
    "    if MODE==modes.TRAIN:\n",
    "        trainer.fit(model,train_dataloaders=datamodule.train_dataloader(),val_dataloaders=datamodule.val_dataloader())\n",
    "        trainer.test(model,dataloaders=datamodule.val_dataloader())\n",
    "\n",
    "        if SAVE_MODEL:\n",
    "            torch.save(trainer.model.state_dict(), MODEL_PATH)\n",
    "        \n",
    "    elif MODE==modes.TEST:\n",
    "        trainer.test(model,dataloaders=datamodule.test_dataloader())\n",
    "           \n",
    "    elif MODE==modes.PREDICT:\n",
    "        predictions = trainer.predict(model,dataloaders=datamodule.predict_dataloader())\n",
    "\n",
    "        for batch in predictions: #Save predictions\n",
    "            batch = pd.DataFrame(torch.softmax(batch,dim=1))\n",
    "            batch.to_csv(f\"{save_dir}/{PREDICT_DATASET.name.lower()}_predictions.csv\",mode='a', index=False, header=False)\n",
    "\n",
    "\n",
    "    if MODE != modes.PREDICT:\n",
    "        #Save cleaned up logs file to Metrics folder & save graph\n",
    "        save_metrics_from_logger(MODEL_ID,PATHS[\"LOG_PATH\"],PATHS['METRICS_PATH'],version=run,mode=MODE.name.lower(),save=True)  \n",
    "        if MODE==modes.TRAIN:\n",
    "            plot_train_metrics(MODEL_ID,PATHS['METRICS_PATH'],version=run,show=False,save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6793"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dereference all objects, clear cuda cache and run garbage collection\n",
    "datamodule=None\n",
    "model=None\n",
    "trainer=None\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mphys-galaxy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
