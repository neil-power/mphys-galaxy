{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_PATH = \"lightning_logs\"\n",
    "METRICS_PATH = \"../Metrics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_folder(save_dir):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "def save_metrics_from_logger(model_id,version=0,save=True):\n",
    "    metrics = pd.read_csv(f\"{LOG_PATH}/{model_id}/version_{version}_train/metrics.csv\")\n",
    "    metrics = metrics.drop(['train_loss_step',\t'train_acc_step',\t'train_calibration_error_step'],axis=1)\n",
    "    metrics = metrics.groupby(metrics['epoch']).first()\n",
    "    if save:\n",
    "        save_dir = f\"{METRICS_PATH}/{model_id}/version_{version}\"\n",
    "        check_folder(save_dir)\n",
    "        metrics.to_csv(f\"{save_dir}/train_metrics.csv\")\n",
    "    return metrics\n",
    "\n",
    "def get_metrics_from_csv(model_id,version=0,save=True):\n",
    "    metrics = pd.read_csv(f\"{METRICS_PATH}/{model_id}/version_{version}/train_metrics.csv\")\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(model_id,version=0,show=False,save=True):\n",
    "    metrics = get_metrics_from_csv(model_id,version)\n",
    "    train_loss = metrics['train_loss_epoch']\n",
    "    train_acc = metrics['train_acc_epoch']\n",
    "    val_loss = metrics['val_loss']\n",
    "    val_acc = metrics['val_acc']\n",
    "    epoch = metrics.index\n",
    "\n",
    "    fig = plt.figure(figsize=(9,4))\n",
    "    ax1 = fig.add_subplot(221) \n",
    "    ax1.plot(epoch,train_loss,linestyle='-', c='orange')\n",
    "    ax1.set_xlabel('Step')\n",
    "    ax1.set_ylabel('Train Loss')\n",
    "\n",
    "    ax2 = fig.add_subplot(222) \n",
    "    ax2.plot(epoch,val_loss,linestyle='-', c='violet')\n",
    "    ax2.set_xlabel('Step')\n",
    "    ax2.set_ylabel('Validation Loss')\n",
    "\n",
    "    ax3 = fig.add_subplot(223) \n",
    "    ax3.plot(epoch,train_acc,linestyle='-', c='red')\n",
    "    ax3.set_xlabel('Step')\n",
    "    ax3.set_ylabel('Train Accuracy')\n",
    "\n",
    "    ax4 = fig.add_subplot(224) \n",
    "    ax4.plot(epoch,val_acc,linestyle='-', c='green')\n",
    "    ax4.set_xlabel('Step')\n",
    "    ax4.set_ylabel('Validation Accuracy')\n",
    "    #plt.tight_layout()\n",
    "    #fig.suptitle(f\"{run_name} (Loss: {test_loss:.2f}, Acc: {test_acc:.2%})\")\n",
    "    fig.suptitle(model_id)\n",
    "    if save:\n",
    "        save_dir = f\"{METRICS_PATH}/{model_id}/version_{version}\"\n",
    "        check_folder(save_dir)\n",
    "        plt.savefig(f'{save_dir}/metrics.png',dpi=200)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate cleaned up csv files for each run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with G_ResNet18_cut_dataset_repeat, run 3\n",
      "Error with G_ResNet18_cut_dataset_repeat, run 4\n"
     ]
    }
   ],
   "source": [
    "model_ids = ['G_LeNet_cut_dataset_repeat','LeNet_cut_dataset_repeat','resnet18_cut_dataset_repeat','jiaresnet50_cut_dataset_repeat','jiaresnet50_cut_dataset_repeat','G_ResNet18_cut_dataset_repeat']\n",
    "\n",
    "for model in model_ids:\n",
    "    for run in range(5):\n",
    "        #print(f\"{model}, run {run}\")\n",
    "        try:\n",
    "            save_metrics_from_logger(model,version=run)\n",
    "        except:\n",
    "            print(f\"Error with {model}, run {run}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create graphs for each run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in model_ids:\n",
    "    for run in range(5):\n",
    "        #print(f\"{model}, run {run}\")\n",
    "        try:\n",
    "            plot_metrics(model,version=run,show=False)\n",
    "        except:\n",
    "            print(f\"Error with {model}, run {run}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get best loss epoch for each run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G_LeNet_cut_dataset_repeat\n",
      "Loss: 0.7068 ± 0.0343\n",
      "Accuracy: 83.39% ± 1.79%\n",
      "ECE: 0.1835 ± 0.0106\n",
      "C Viol: 0.5079 ± 0.4110\n",
      "\n",
      "LeNet_cut_dataset_repeat\n",
      "Loss: 0.8562 ± 0.0341\n",
      "Accuracy: 70.61% ± 3.42%\n",
      "ECE: 0.1421 ± 0.0146\n",
      "C Viol: nan ± nan\n",
      "\n",
      "resnet18_cut_dataset_repeat\n",
      "Loss: 0.5290 ± 0.0003\n",
      "Accuracy: 98.16% ± 0.12%\n",
      "ECE: 0.2273 ± 0.0023\n",
      "C Viol: 0.4963 ± 0.1710\n",
      "\n",
      "jiaresnet50_cut_dataset_repeat\n",
      "Loss: 0.5310 ± 0.0034\n",
      "Accuracy: 97.65% ± 0.82%\n",
      "ECE: 0.2232 ± 0.0066\n",
      "C Viol: 0.2887 ± 0.0618\n",
      "\n",
      "jiaresnet50_cut_dataset_repeat\n",
      "Loss: 0.5310 ± 0.0034\n",
      "Accuracy: 97.65% ± 0.82%\n",
      "ECE: 0.2232 ± 0.0066\n",
      "C Viol: 0.2887 ± 0.0618\n",
      "\n",
      "Error with G_ResNet18_cut_dataset_repeat, run 3\n",
      "Error with G_ResNet18_cut_dataset_repeat, run 4\n",
      "G_ResNet18_cut_dataset_repeat\n",
      "Loss: 0.6047 ± 0.1359\n",
      "Accuracy: 92.19% ± 11.39%\n",
      "ECE: 0.2100 ± 0.0293\n",
      "C Viol: nan ± nan\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in model_ids:\n",
    "    best_losses = []\n",
    "    best_accs = []\n",
    "    best_eces = []\n",
    "    best_chiralities = []\n",
    "    for run in range(5):\n",
    "        try:\n",
    "            metrics = get_metrics_from_csv(model,version=run)\n",
    "            best_loss_epoch = metrics['val_loss'].argmin()\n",
    "            best_losses.append(metrics['val_loss'][best_loss_epoch])\n",
    "            best_accs.append(metrics['val_acc'][best_loss_epoch])\n",
    "            best_eces.append(metrics['val_calibration_error'][best_loss_epoch])\n",
    "            best_chiralities.append((metrics['val_chirality_violation'][best_loss_epoch]))\n",
    "        except:\n",
    "            print(f\"Error with {model}, run {run}\")\n",
    "    print(model)\n",
    "    print(f\"Loss: {np.average(best_losses):.4f} ± {np.std(best_losses):.4f}\")\n",
    "    print(f\"Accuracy: {np.average(best_accs):.2%} ± {np.std(best_accs):.2%}\")\n",
    "    print(f\"ECE: {np.average(best_eces):.4f} ± {np.std(best_eces):.4f}\")\n",
    "    print(f\"C Viol: {np.average(best_chiralities):.4f} ± {np.std(best_chiralities):.4f}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mphys-galaxy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
