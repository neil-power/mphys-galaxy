{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from escnn import gspaces\n",
    "from escnn import nn as enn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mask(s, margin=2, dtype=torch.float32):\n",
    "    mask = torch.zeros(1, 1, s, s, dtype=dtype)\n",
    "    c = (s-1) / 2\n",
    "    t = (c - margin/100.*c)**2\n",
    "    sig = 2.\n",
    "    for x in range(s):\n",
    "        for y in range(s):\n",
    "            r = (x - c) ** 2 + (y - c) ** 2\n",
    "            if r > t:\n",
    "                mask[..., x, y] = math.exp((t - r)/sig**2)\n",
    "            else:\n",
    "                mask[..., x, y] = 1.\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgsize = 160 #pixels along one side\n",
    "num_classes = 3\n",
    "num_channels =3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaLeNet(nn.Module):\n",
    "    def __init__(self, in_chan, out_chan, imsize, kernel_size=5, N=None):\n",
    "        super(VanillaLeNet, self).__init__()\n",
    "        \n",
    "        z = 0.5*(imsize - 2)\n",
    "        z = int(0.5*(z - 2))\n",
    "        \n",
    "        self.mask = build_mask(imsize, margin=1)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_chan, 6, kernel_size, padding=1)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size, padding=1)\n",
    "        self.fc1   = nn.Linear(16*z*z, 120)\n",
    "        self.fc2   = nn.Linear(120, 84)\n",
    "        self.fc3   = nn.Linear(84, out_chan)\n",
    "        self.drop  = nn.Dropout(p=0.5)\n",
    "        \n",
    "        # dummy parameter for tracking device\n",
    "        self.dummy = nn.Parameter(torch.empty(0))\n",
    "        \n",
    "    def loss(self,p,y):\n",
    "        \n",
    "        # check device for model:\n",
    "        device = self.dummy.device\n",
    "        \n",
    "        # p : softmax(x)\n",
    "        loss_fnc = nn.NLLLoss().to(device=device)\n",
    "        loss = loss_fnc(torch.log(p),y)\n",
    "        \n",
    "        return loss\n",
    "     \n",
    "    def enable_dropout(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Dropout):\n",
    "                m.train()\n",
    "\n",
    "        return\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # check device for model:\n",
    "        device = self.dummy.device\n",
    "        mask = self.mask.to(device=device)\n",
    "        \n",
    "        x = x*mask\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        x = x.view(x.size()[0], -1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.fc3(x)\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNSteerableLeNet(nn.Module):\n",
    "    def __init__(self, in_chan, out_chan, imsize, kernel_size=5, N=8):\n",
    "        super(CNSteerableLeNet, self).__init__()\n",
    "        \n",
    "        z = 0.5*(imsize - 2)\n",
    "        z = int(0.5*(z - 2))\n",
    "        \n",
    "        self.r2_act = gspaces.Rot2dOnR2(N)\n",
    "        \n",
    "        in_type = enn.FieldType(self.r2_act, [self.r2_act.trivial_repr])\n",
    "        self.input_type = in_type\n",
    "        \n",
    "        out_type = enn.FieldType(self.r2_act, 6*[self.r2_act.regular_repr])\n",
    "        self.mask = enn.MaskModule(in_type, imsize, margin=1)\n",
    "        self.conv1 = enn.R2Conv(in_type, out_type, kernel_size=5, padding=1, bias=False)\n",
    "        self.relu1 = enn.ReLU(out_type, inplace=True)\n",
    "        self.pool1 = enn.PointwiseMaxPoolAntialiased(out_type, kernel_size=2)\n",
    "\n",
    "        in_type = self.pool1.out_type\n",
    "        out_type = enn.FieldType(self.r2_act, 16*[self.r2_act.regular_repr])\n",
    "        self.conv2 = enn.R2Conv(in_type, out_type, kernel_size=5, padding=1, bias=False)\n",
    "        self.relu2 = enn.ReLU(out_type, inplace=True)\n",
    "        self.pool2 = enn.PointwiseMaxPoolAntialiased(out_type, kernel_size=2)\n",
    "        \n",
    "        self.gpool = enn.GroupPooling(out_type)\n",
    "\n",
    "        self.fc1   = nn.Linear(16*z*z, 120)\n",
    "        self.fc2   = nn.Linear(120, 84)\n",
    "        self.fc3   = nn.Linear(84, out_chan)\n",
    "        \n",
    "        self.drop  = nn.Dropout(p=0.5)\n",
    "        \n",
    "        # dummy parameter for tracking device\n",
    "        self.dummy = nn.Parameter(torch.empty(0))\n",
    "        \n",
    "        \n",
    "    def loss(self,p,y):\n",
    "        \n",
    "        # check device for model:\n",
    "        device = self.dummy.device\n",
    "        \n",
    "        # p : softmax(x)\n",
    "        loss_fnc = nn.NLLLoss().to(device=device)\n",
    "        loss = loss_fnc(torch.log(p),y)\n",
    "        \n",
    "        return loss\n",
    "     \n",
    "    def enable_dropout(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Dropout):\n",
    "                m.train()\n",
    "\n",
    "        return\n",
    "      \n",
    "      \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = enn.GeometricTensor(x, self.input_type)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.gpool(x)\n",
    "        x = x.tensor\n",
    "        \n",
    "        x = x.view(x.size()[0], -1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.fc3(x)\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hii = VanillaLeNet(num_channels,num_classes,imgsize+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to train the model on data\n",
    "def train(model, trainloader, optimiser, device):\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (data, labels) in enumerate(trainloader):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        #calculate train loss\n",
    "        p_y = model(data)\n",
    "        loss_criterion = nn.CrossEntropyLoss()\n",
    "        labels = labels.type(torch.LongTensor) \n",
    "        labels = labels.to(device)\n",
    "        loss = loss_criterion(p_y, labels)\n",
    "            \n",
    "        train_loss += loss.item() * data.size(0)\n",
    "\n",
    "        #feed the loss back\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "    train_loss /= len(trainloader.dataset)\n",
    "    return train_loss\n",
    "\n",
    "#function to test the model on data\n",
    "def validate(model, testloader, device):\n",
    "    prediction=[]\n",
    "    target=[]\n",
    "    py_ten=torch.empty((0,3), dtype=torch.float64)\n",
    "    correct = 0\n",
    "    test_loss = 0.0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, labels) in enumerate(testloader):\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "            p_y = model(data)\n",
    "            # if METHOD:\n",
    "            #     #tensor for probabilities, for ROC score\n",
    "            #     py_ten = torch.cat((py_ten, p_y), 0)\n",
    "\n",
    "            #calculate test loss\n",
    "            loss_criterion = nn.CrossEntropyLoss()\n",
    "            labels = labels.type(torch.LongTensor)\n",
    "            labels = labels.to(device)\n",
    "            loss = loss_criterion(p_y, labels)\n",
    "                \n",
    "            test_loss += loss.item() * data.size(0)\n",
    "            \n",
    "            #values for metrics\n",
    "            preds = p_y.argmax(dim=1, keepdim=True)\n",
    "            correct += preds.eq(labels.view_as(preds)).sum().item()\n",
    "            prediction+=preds.squeeze(1).tolist()\n",
    "            target+=labels.tolist()\n",
    "        test_loss /= len(testloader.dataset)\n",
    "        accuracy = correct / len(testloader.dataset)\n",
    "\n",
    "        #calculate ROC score metric\n",
    "        # if METHOD:\n",
    "        #     scale_prob = nn.Softmax(dim=1)(py_ten).numpy()\n",
    "        #     roc_auc = roc_auc_score(np.array(target), scale_prob, multi_class=\"ovr\")\n",
    "        #     recall = recall_score(target, prediction, average='weighted')\n",
    "        #     f1 = f1_score(target, prediction, average='weighted')\n",
    "        #     precision = precision_score(target, prediction, average='weighted')\n",
    "        # else:\n",
    "        roc_auc = roc_auc_score(target, prediction)\n",
    "        recall = recall_score(target, prediction)\n",
    "        f1 = f1_score(target, prediction)\n",
    "        precision = precision_score(target, prediction)\n",
    "    return test_loss, accuracy, roc_auc, prediction, target, recall, f1, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to run the entire model train/test loop\n",
    "def run_func():\n",
    "    #split the data into train, test and validation\n",
    "    # X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X, y,test_size=0.3)\n",
    "    # X_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)\n",
    "\n",
    "    # X_train_tensor = torch.from_numpy(X_train).float()\n",
    "    # Y_train_tensor = torch.from_numpy(Y_train.astype(\"float64\")).float()\n",
    "\n",
    "    # X_val_tensor = torch.from_numpy(X_val).float()\n",
    "    # Y_val_tensor = torch.from_numpy(Y_val.astype(\"float64\")).float()\n",
    "\n",
    "    # X_test_tensor = torch.from_numpy(X_test).float()\n",
    "    # Y_test_tensor = torch.from_numpy(Y_test.astype(\"float64\")).float()\n",
    "\n",
    "    # trainset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "    # valset = TensorDataset(X_val_tensor, Y_val_tensor)\n",
    "    # testset = TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "\n",
    "    # train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "    # val_loader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=True)\n",
    "    # test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # #create the model\n",
    "    # model = CustomNN(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "    # #optimizer and learning rate scheduler\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay = weight_decay)#0.005, #0.01\n",
    "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, patience=2, factor=0.9)\n",
    "\n",
    "    # #array to store metrics\n",
    "    # result_arr = np.zeros((num_epochs,8))\n",
    "\n",
    "    # _bestloss = 1.\n",
    "    # bestepoch = 0\n",
    "    # #NN learning\n",
    "    # for epoch in range(num_epochs):\n",
    "    \n",
    "    #     train_loss = train(model, train_loader, optimizer, device)\n",
    "    #     val_loss, accuracy, roc_auc, prediction, target,  recall, f1, precision = validate(model, val_loader, device)\n",
    "        \n",
    "    #     scheduler.step(val_loss)\n",
    "\n",
    "    #     if early_stopping and val_loss<_bestloss:\n",
    "    #         _bestloss = val_loss\n",
    "    #         torch.save(model.state_dict(), modfile)\n",
    "    #         best_epoch = epoch\n",
    "    #         cm = confusion_matrix(target, prediction,normalize = 'true')\n",
    "\n",
    "    #     #set output row\n",
    "    #     results = [epoch, train_loss, val_loss, accuracy, roc_auc,  recall, f1, precision]\n",
    "    #     result_arr[epoch] = results\n",
    "\n",
    "    #     #print epocch results\n",
    "    #     if not QUIET:\n",
    "    #         print('Epoch: {}, Validation Loss: {:4f}, Validation Accuracy: {:4f}'.format(epoch, val_loss, accuracy))\n",
    "    #         print('Current learning rate is: {}'.format(optimizer.param_groups[0]['lr']))\n",
    "    \n",
    "    # test_arr=[]\n",
    "    # if TEST_USE:\n",
    "    #     #Test data run\n",
    "    #     bestmodel=CustomNN(input_size, hidden_size, output_size).to(device)\n",
    "    #     bestmodel.load_state_dict(torch.load('modfile.pt'))\n",
    "    #     test_loss, test_accuracy, test_auc, prediction2,target2,  recall2, f12, precision2 = validate(bestmodel, test_loader, device)\n",
    "    #     test_arr=[test_loss, test_accuracy, test_auc, prediction2,target2,  recall2, f12, precision2]\n",
    "\n",
    "    # if not early_stopping:\n",
    "    #     torch.save(model.state_dict(), modfile)\n",
    "    #     best_epoch = -1\n",
    "    #     cm = confusion_matrix(target, prediction,normalize = 'true')\n",
    "\n",
    "    # return result_arr, cm, test_arr, best_epoch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Galaxy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
